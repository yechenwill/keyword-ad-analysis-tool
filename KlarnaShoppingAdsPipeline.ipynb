{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31c2a019-5f3a-4c72-80f7-04a3eb075aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "import paramiko\n",
    "import numpy as np\n",
    "import gzip\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime, timezone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9a5f1d-67cc-4ac0-8210-445982a17367",
   "metadata": {},
   "source": [
    "## Download Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8639af-e732-4236-ac07-f247b4b882b7",
   "metadata": {},
   "source": [
    "### Most Advs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab9717f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bloomingdales: File '20250609_Bloomingdales_PLA.csv' on SFTP was last modified on None. Skipping download.\n",
      "Verizon: File 'verizon_devices_admarketplace.csv' on SFTP was last modified on None. Skipping download.\n",
      "BedBathBeyond: File '20250609_BedBathAndBeyond_PLA.csv.gz' on SFTP was last modified on None. Skipping download.\n",
      "HarryDavid: Local file 'HarryDavid_PLA.csv' already exists and was modified today (2025-06-09 07:50:25.216850). Skipping download.\n",
      "TommyBahama: File '20250609_TommyBahama_PLA.csv' on SFTP was last modified on None. Skipping download.\n",
      "Houzz: File 'houzz_full_catalog.txt.gz' on SFTP was last modified on 2025-03-30 18:56:56+00:00. Skipping download.\n",
      "Zappos: File 'zapoos_adsmarketplace.txt.gz' on SFTP was last modified on 2025-04-25 19:24:34+00:00. Skipping download.\n",
      "HomeDepot: File '20250609_TheHomeDepot_029A.csv.gz' on SFTP was last modified on None. Skipping download.\n",
      "NewBalance: Local file 'NewBalance_PLA.csv' already exists and was modified today (2025-06-09 07:50:27.317199). Skipping download.\n",
      "CPU times: user 76.2 ms, sys: 10.7 ms, total: 86.9 ms\n",
      "Wall time: 907 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import paramiko\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# SFTP credentials and connection details\n",
    "sftp_host = 'ftp.admarketplace.net'\n",
    "sftp_port = 8022\n",
    "username = 'ywang'\n",
    "password = '123456789'  # Recommend using environment variables for security\n",
    "\n",
    "# Dictionary to store advertiser SFTP paths, local directories, and file naming conventions\n",
    "advertisers = {\n",
    "    'Bloomingdales': {\n",
    "        'sftp_path': '/sftp/l_bloomingdales/files/',\n",
    "        'local_path': '/Volumes/T9/AMP/KlarnaShoppingAds/Bloomingdales/',\n",
    "        'file_pattern': lambda date: f'{date}_Bloomingdales_PLA.csv',\n",
    "        'final_name': 'Bloomingdales_PLA.csv'\n",
    "    },\n",
    "    'Verizon': {\n",
    "        'sftp_path': '/sftp/l_verizon/files/',\n",
    "        'local_path': '/Volumes/T9/AMP/KlarnaShoppingAds/Verizon/',\n",
    "        'file_pattern': 'verizon_devices_admarketplace.csv',\n",
    "        'final_name': 'Verizon_PLA.csv'\n",
    "    },\n",
    "    'BedBathBeyond': {\n",
    "        'sftp_path': '/sftp/l_BedBathBeyond/files/',\n",
    "        'local_path': '/Volumes/T9/AMP/KlarnaShoppingAds/BedBathBeyond/',\n",
    "        'file_pattern': lambda date: f'{date}_BedBathAndBeyond_PLA.csv.gz',\n",
    "        'final_name': 'BedBathBeyond_PLA.csv.gz'\n",
    "    },\n",
    "    'HarryDavid': {\n",
    "        'sftp_path': '/sftp/l_HarryDavid/files/',\n",
    "        'local_path': '/Volumes/T9/AMP/KlarnaShoppingAds/HarryDavid/',\n",
    "        'file_pattern': 'hd_admarketplace.csv',\n",
    "        'final_name': 'HarryDavid_PLA.csv'\n",
    "    },\n",
    "    'TommyBahama': {\n",
    "        'sftp_path': '/sftp/l_Tommybahama/files/',\n",
    "        'local_path': '/Volumes/T9/AMP/KlarnaShoppingAds/TommyBahama/',\n",
    "        'file_pattern': lambda date: f'{date}_TommyBahama_PLA.csv',\n",
    "        'final_name': 'TommyBahama_PLA.csv'\n",
    "    },\n",
    "    'Houzz': {\n",
    "        'sftp_path': '/sftp/l_houzz/files/pla_feed/',\n",
    "        'local_path': '/Volumes/T9/AMP/KlarnaShoppingAds/Houzz/',\n",
    "        'file_pattern': 'houzz_full_catalog.txt.gz',\n",
    "        'final_name': 'Houzz_PLA.txt.gz'\n",
    "    },\n",
    "    'Zappos': {\n",
    "        'sftp_path': '/sftp/l_Zappos-1/files/',\n",
    "        'local_path': '/Volumes/T9/AMP/KlarnaShoppingAds/Zappos/',\n",
    "        'file_pattern': 'zapoos_adsmarketplace.txt.gz',\n",
    "        'final_name': 'Zappos_PLA.txt.gz'\n",
    "    },\n",
    "    'HomeDepot': {\n",
    "        'sftp_path': '/sftp/l_homedepot/files/',\n",
    "        'local_path': '/Volumes/T9/AMP/KlarnaShoppingAds/TheHomeDepot/',\n",
    "        'file_pattern': lambda date: f'{date}_TheHomeDepot_029A.csv.gz',\n",
    "        'final_name': 'HomeDepot_PLA.csv.gz'\n",
    "    },\n",
    "    'NewBalance': {\n",
    "        'sftp_path': '/sftp/l_Newbalance/files/',\n",
    "        'local_path': '/Volumes/T9/AMP/KlarnaShoppingAds/NewBalance/',\n",
    "        'file_pattern': lambda date: f'{date}_NewBalance_ PLA.csv',\n",
    "        'final_name': 'NewBalance_PLA.csv'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Helper function to check if a file exists locally and was modified today\n",
    "def is_local_file_modified_today(local_file_path):\n",
    "    if os.path.exists(local_file_path):\n",
    "        modification_time = datetime.fromtimestamp(os.path.getmtime(local_file_path))\n",
    "        return modification_time.date() == datetime.now().date(), modification_time\n",
    "    return False, None\n",
    "\n",
    "# Helper function to check if the file on the SFTP server was modified today\n",
    "def is_sftp_file_modified_today(sftp, file_name):\n",
    "    try:\n",
    "        file_attr = sftp.stat(file_name)\n",
    "        modification_time = datetime.fromtimestamp(file_attr.st_mtime, timezone.utc)\n",
    "        return modification_time.date() == datetime.now(timezone.utc).date(), modification_time\n",
    "    except FileNotFoundError:\n",
    "        return False, None\n",
    "\n",
    "# Establish SFTP connection once\n",
    "try:\n",
    "    transport = paramiko.Transport((sftp_host, sftp_port))\n",
    "    transport.connect(username=username, password=password)\n",
    "    sftp = paramiko.SFTPClient.from_transport(transport)\n",
    "\n",
    "    # Get the current date string\n",
    "    current_date = datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "    # Step 1: Process all advertisers\n",
    "    for advertiser, paths in advertisers.items():\n",
    "        sftp_folder = paths['sftp_path']\n",
    "        local_folder = paths['local_path']\n",
    "        file_pattern = paths['file_pattern']\n",
    "        final_filename = paths['final_name']  # Static name for overwriting\n",
    "\n",
    "        # Generate file name for SFTP (dynamic or static)\n",
    "        sftp_filename = file_pattern(current_date) if callable(file_pattern) else file_pattern\n",
    "        local_file_path = os.path.join(local_folder, final_filename)  # Always save as fixed name\n",
    "\n",
    "        # Ensure local folder exists\n",
    "        os.makedirs(local_folder, exist_ok=True)\n",
    "\n",
    "        # Check if the file exists locally and was updated today\n",
    "        local_exists, local_mod_time = is_local_file_modified_today(local_file_path)\n",
    "        if local_exists:\n",
    "            print(f\"{advertiser}: Local file '{final_filename}' already exists and was modified today ({local_mod_time}). Skipping download.\")\n",
    "            continue  # Skip if already downloaded today\n",
    "\n",
    "        # Navigate to SFTP folder\n",
    "        sftp.chdir(sftp_folder)\n",
    "\n",
    "        # Check if the SFTP file was modified today\n",
    "        sftp_updated_today, sftp_mod_time = is_sftp_file_modified_today(sftp, sftp_filename)\n",
    "        if not sftp_updated_today:\n",
    "            print(f\"{advertiser}: File '{sftp_filename}' on SFTP was last modified on {sftp_mod_time}. Skipping download.\")\n",
    "            continue  # Skip if file not updated today\n",
    "\n",
    "        try:\n",
    "            # Download the file and save it with a fixed name (overwrite)\n",
    "            sftp.get(sftp_filename, local_file_path)\n",
    "            print(f\"{advertiser}: Downloaded '{sftp_filename}' and saved as '{final_filename}' (Overwritten).\")\n",
    "        except Exception as e:\n",
    "            print(f\"{advertiser}: Failed to download '{sftp_filename}'. Error: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while downloading: {e}\")\n",
    "\n",
    "finally:\n",
    "    if sftp:\n",
    "        sftp.close()\n",
    "    if transport:\n",
    "        transport.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb287401",
   "metadata": {},
   "source": [
    "Under Armour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6abd6478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnderArmour: Local file 'UnderArmour_PLA.csv' already exists and was modified today (2025-06-09 07:50:13.119061). Skipping download.\n",
      "CPU times: user 5.38 ms, sys: 3.56 ms, total: 8.94 ms\n",
      "Wall time: 460 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import paramiko\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# SFTP credentials and connection details\n",
    "sftp_host = 'sftp.admarketplace.net'\n",
    "sftp_port = 22\n",
    "username = 'underarmour'\n",
    "password = 'supports-GALE-mobility-postman'  # Recommend using environment variables for security\n",
    "\n",
    "# Dictionary to store advertiser SFTP paths, local directories, and file naming conventions\n",
    "advertisers = {\n",
    "    'UnderArmour': {\n",
    "        'sftp_path': '/files/',\n",
    "        'local_path': '/Volumes/T9/AMP/KlarnaShoppingAds/UnderArmour/',\n",
    "        'file_pattern': 'UA_AMP.csv',\n",
    "        'final_name': 'UnderArmour_PLA.csv'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Helper function to check if a file exists locally and was modified today\n",
    "def is_local_file_modified_today(local_file_path):\n",
    "    if os.path.exists(local_file_path):\n",
    "        modification_time = datetime.fromtimestamp(os.path.getmtime(local_file_path))\n",
    "        return modification_time.date() == datetime.now().date(), modification_time\n",
    "    return False, None\n",
    "\n",
    "# Helper function to check if the file on the SFTP server was modified today\n",
    "def is_sftp_file_modified_today(sftp, file_name):\n",
    "    try:\n",
    "        file_attr = sftp.stat(file_name)\n",
    "        modification_time = datetime.fromtimestamp(file_attr.st_mtime, timezone.utc)\n",
    "        return modification_time.date() == datetime.now(timezone.utc).date(), modification_time\n",
    "    except FileNotFoundError:\n",
    "        return False, None\n",
    "\n",
    "# Establish SFTP connection once\n",
    "try:\n",
    "    transport = paramiko.Transport((sftp_host, sftp_port))\n",
    "    transport.connect(username=username, password=password)\n",
    "    sftp = paramiko.SFTPClient.from_transport(transport)\n",
    "\n",
    "    # Get the current date string\n",
    "    current_date = datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "    # Step 1: Process all advertisers\n",
    "    for advertiser, paths in advertisers.items():\n",
    "        sftp_folder = paths['sftp_path']\n",
    "        local_folder = paths['local_path']\n",
    "        file_pattern = paths['file_pattern']\n",
    "        final_filename = paths['final_name']  # Static name for overwriting\n",
    "\n",
    "        # Generate file name for SFTP (dynamic or static)\n",
    "        sftp_filename = file_pattern(current_date) if callable(file_pattern) else file_pattern\n",
    "        local_file_path = os.path.join(local_folder, final_filename)  # Always save as fixed name\n",
    "\n",
    "        # Ensure local folder exists\n",
    "        os.makedirs(local_folder, exist_ok=True)\n",
    "\n",
    "        # Check if the file exists locally and was updated today\n",
    "        local_exists, local_mod_time = is_local_file_modified_today(local_file_path)\n",
    "        if local_exists:\n",
    "            print(f\"{advertiser}: Local file '{final_filename}' already exists and was modified today ({local_mod_time}). Skipping download.\")\n",
    "            continue  # Skip if already downloaded today\n",
    "\n",
    "        # Navigate to SFTP folder\n",
    "        sftp.chdir(sftp_folder)\n",
    "\n",
    "        # Check if the SFTP file was modified today\n",
    "        sftp_updated_today, sftp_mod_time = is_sftp_file_modified_today(sftp, sftp_filename)\n",
    "        if not sftp_updated_today:\n",
    "            print(f\"{advertiser}: File '{sftp_filename}' on SFTP was last modified on {sftp_mod_time}. Skipping download.\")\n",
    "            continue  # Skip if file not updated today\n",
    "\n",
    "        try:\n",
    "            # Download the file and save it with a fixed name (overwrite)\n",
    "            sftp.get(sftp_filename, local_file_path)\n",
    "            print(f\"{advertiser}: Downloaded '{sftp_filename}' and saved as '{final_filename}' (Overwritten).\")\n",
    "        except Exception as e:\n",
    "            print(f\"{advertiser}: Failed to download '{sftp_filename}'. Error: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while downloading: {e}\")\n",
    "\n",
    "finally:\n",
    "    if sftp:\n",
    "        sftp.close()\n",
    "    if transport:\n",
    "        transport.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72749a91-3280-489f-9a87-f23b2e309b71",
   "metadata": {},
   "source": [
    "<!-- ### Spanx -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f592b1-7015-46c2-8fd5-977b6db6f39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanx: File downloaded from SFTP and saved locally as C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/Spanx/Spanx_AdMarketplace.csv.gz\n"
     ]
    }
   ],
   "source": [
    "# # SFTP credentials and connection details\n",
    "# sftp_host = 'sftpgo.feedonomics.com'\n",
    "# sftp_port = 22  # Default port for SFTP\n",
    "# username = 'fdx_eb4e950355841'\n",
    "# password = 'c0dff59b60d30e836cd6a5f0'  # Recommend using environment variables for credentials\n",
    "\n",
    "# advertisers = {\n",
    "#     'Spanx': {\n",
    "#         'sftp_path': '/incoming/',\n",
    "#         'local_path': '/Volumes/T9/AMP/KlarnaShoppingAds/Spanx/',\n",
    "#         'file_pattern': 'Spanx_AdMarketplace.csv.gz'  # Example: 20241017_Bloomingdales_PLA.csv\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # Helper function to check if a file was modified today\n",
    "# def is_file_modified_today(sftp, file_name):\n",
    "#     file_attr = sftp.stat(file_name)\n",
    "#     modification_time = datetime.fromtimestamp(file_attr.st_mtime, timezone.utc)\n",
    "#     return modification_time.date() == datetime.now(timezone.utc).date(), modification_time\n",
    "\n",
    "# # Helper function to check if the local file already exists and was modified today\n",
    "# def is_local_file_modified_today(local_file_path):\n",
    "#     if os.path.exists(local_file_path):\n",
    "#         modification_time = datetime.fromtimestamp(os.path.getmtime(local_file_path))\n",
    "#         return modification_time.date() == datetime.now().date(), modification_time\n",
    "#     return False, None\n",
    "\n",
    "# # Establish SFTP connection once\n",
    "# try:\n",
    "#     transport = paramiko.Transport((sftp_host, sftp_port))\n",
    "#     transport.connect(username=username, password=password)\n",
    "#     sftp = paramiko.SFTPClient.from_transport(transport)\n",
    "\n",
    "#     # Get the current date string\n",
    "#     current_date = datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "#     # Step 1: Download all files from SFTP into local folders\n",
    "#     for advertiser, paths in advertisers.items():\n",
    "#         sftp_folder = paths['sftp_path']\n",
    "#         local_folder = paths['local_path']\n",
    "#         file_pattern = paths['file_pattern']  # Naming convention for the file\n",
    "\n",
    "#         # Generate the file name based on whether it's a callable (lambda) or a static string\n",
    "#         input_file_name = file_pattern(current_date) if callable(file_pattern) else file_pattern\n",
    "#         local_file_path = os.path.join(local_folder, input_file_name)\n",
    "\n",
    "#         # Ensure the local folder exists, but don't create the file yet\n",
    "#         os.makedirs(local_folder, exist_ok=True)\n",
    "\n",
    "#         # Check if the local file exists and was modified today\n",
    "#         local_exists, local_mod_time = is_local_file_modified_today(local_file_path)\n",
    "#         if local_exists:\n",
    "#             print(f\"{advertiser}: Local file '{input_file_name}' already exists and was modified on {local_mod_time}. Skipping download.\")\n",
    "#             continue\n",
    "\n",
    "#         # Navigate to the advertiser's SFTP directory\n",
    "#         sftp.chdir(sftp_folder)\n",
    "\n",
    "#         # For fixed-name files, check if the file was updated today before downloading\n",
    "#         if not callable(file_pattern):  # For static file names\n",
    "#             sftp_updated_today, sftp_mod_time = is_file_modified_today(sftp, input_file_name)\n",
    "#             if not sftp_updated_today:\n",
    "#                 print(f\"{advertiser}: File '{input_file_name}' on SFTP was last modified on {sftp_mod_time}. Skipping download.\")\n",
    "#                 continue  # Skip download if file was not updated today\n",
    "\n",
    "#         try:\n",
    "#             # Download the file from SFTP to the local system (local file will only be created if download is successful)\n",
    "#             sftp.get(input_file_name, local_file_path)\n",
    "#             print(f\"{advertiser}: File downloaded from SFTP and saved locally as {local_file_path}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"{advertiser}: Failed to download '{input_file_name}'. Error: {e}\")\n",
    "#             # Remove any partially created or empty file after download failure\n",
    "#             if os.path.exists(local_file_path):\n",
    "#                 os.remove(local_file_path)\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"An error occurred while downloading: {e}\")\n",
    "\n",
    "# finally:\n",
    "#     if sftp:\n",
    "#         sftp.close()\n",
    "#     if transport:\n",
    "#         transport.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1496dc-48d6-497f-bb92-94be89f6389f",
   "metadata": {},
   "source": [
    "### Ulta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b77f7dfe-3ae0-482f-acca-5ecd3807559b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded file saved as: C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/Ulta\\feed_20250424161740_33000020.txt\n",
      "File has been saved as TSV at: C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/Ulta\\ulta.tsv\n",
      "CPU times: total: 2.45 s\n",
      "Wall time: 5.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "# Specify the folder path\n",
    "folder_path = '/Volumes/T9/AMP/KlarnaShoppingAds/Ulta'\n",
    "\n",
    "# Generate the current timestamp in 'yyyymmddhhmmss' format for the filename\n",
    "current_timestamp = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "input_file_name = f'feed_{current_timestamp}_33000020.txt'\n",
    "output_file_name = 'ulta.tsv'\n",
    "\n",
    "# Create the full file paths\n",
    "input_file_path = os.path.join(folder_path, input_file_name)\n",
    "output_file_path = os.path.join(folder_path, output_file_name)\n",
    "\n",
    "# URL for the daily download\n",
    "url = \"https://webadapters.channeladvisor.com/CSEAdapter/Default.aspx?pid=V%5bP%5e%5eC%5ePAosvB6Z.X%5b3KePQjFGq_%5bZX2%5bLd%22(%3dsFt4%5b%60%26K2Ic%23)gwz%3d7Z%5eY%5bbI_SQ8DLu_U%2f%26%5ebucR(%3cwz\"\n",
    "\n",
    "# Download the file\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Save the downloaded content to the specified input file\n",
    "    with open(input_file_path, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"Downloaded file saved as: {input_file_path}\")\n",
    "\n",
    "    # Read the content from the text file and save it as TSV\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        \n",
    "    with open(output_file_path, 'w', encoding='utf-8') as tsv_file:\n",
    "        tsv_file.write(content)\n",
    "    \n",
    "    print(f\"File has been saved as TSV at: {output_file_path}\")\n",
    "\n",
    "except requests.HTTPError as e:\n",
    "    print(f\"HTTP error occurred: {e}\")\n",
    "except UnicodeDecodeError:\n",
    "    print(\"Error: Could not decode the file. Please check the file encoding or try using a different encoding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59268399",
   "metadata": {},
   "source": [
    "### UK ADVS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d543649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded file for Sephora_UK saved as: /Volumes/T9/AMP/KlarnaShoppingAds/Sephora/feed_20250609072943.tsv\n",
      "Processed and saved: /Volumes/T9/AMP/KlarnaShoppingAds/Sephora/sephora_uk.tsv\n",
      "Downloaded file for MyProtein_UK saved as: /Volumes/T9/AMP/KlarnaShoppingAds/My Protein/UK/feed_20250609072943.csv\n",
      "Processed and saved: /Volumes/T9/AMP/KlarnaShoppingAds/My Protein/UK/myprotein_uk.tsv\n",
      "Downloaded file for LookFantastic_UK saved as: /Volumes/T9/AMP/KlarnaShoppingAds/Look Fantastic/UK/feed_20250609072943.csv\n",
      "Warning: LookFantastic_UK data appears to be in one column. Attempting to split...\n",
      "Processed and saved: /Volumes/T9/AMP/KlarnaShoppingAds/Look Fantastic/UK/lookfantastic_uk.tsv\n",
      "Downloaded file for LookFantastic_FR saved as: /Volumes/T9/AMP/KlarnaShoppingAds/Look Fantastic/FR/feed_20250609072943.csv\n",
      "Warning: LookFantastic_FR data appears to be in one column. Attempting to split...\n",
      "Processed and saved: /Volumes/T9/AMP/KlarnaShoppingAds/Look Fantastic/FR/lookfantastic_fr.tsv\n",
      "Downloaded file for LookFantastic_IT saved as: /Volumes/T9/AMP/KlarnaShoppingAds/Look Fantastic/IT/feed_20250609072943.csv\n",
      "Warning: LookFantastic_IT data appears to be in one column. Attempting to split...\n",
      "Processed and saved: /Volumes/T9/AMP/KlarnaShoppingAds/Look Fantastic/IT/lookfantastic_it.tsv\n",
      "Downloaded file for NewLook_UK saved as: /Volumes/T9/AMP/KlarnaShoppingAds/New Look/feed_20250609072943.txt\n",
      "Processed and saved: /Volumes/T9/AMP/KlarnaShoppingAds/New Look/newlook_uk.tsv\n",
      "Downloaded file for Vodafone_UK saved as: /Volumes/T9/AMP/KlarnaShoppingAds/Vodafone/feed_20250609072943.txt\n",
      "Processed and saved: /Volumes/T9/AMP/KlarnaShoppingAds/Vodafone/vodafone_uk.tsv\n",
      "All downloads complete.\n",
      "CPU times: user 5.58 s, sys: 1.55 s, total: 7.13 s\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Dictionary of advertisers with corrected URLs\n",
    "advertisers = {\n",
    "    \"Sephora_UK\": {\n",
    "        \"folder\": \"/Volumes/T9/AMP/KlarnaShoppingAds/Sephora\",\n",
    "        \"url\": \"http://files-as.intelligentreach.com/feedexports/1a627511-1fcb-4ea2-885c-b849e10a8688/Feel_Unique_UK_Admarketplace.tsv\"\n",
    "    },\n",
    "    \"MyProtein_UK\": {\n",
    "        \"folder\": \"/Volumes/T9/AMP/KlarnaShoppingAds/My Protein/UK\",\n",
    "        \"url\": \"https://admarketplace:KIBC07LjHMTTcSdhRzmd@productfeeds.thehut.net/feeds/Myprotein_uk_admarketplace_2.csv\"\n",
    "    },\n",
    "    \"LookFantastic_UK\": {\n",
    "        \"folder\": \"/Volumes/T9/AMP/KlarnaShoppingAds/Look Fantastic/UK\",\n",
    "        \"url\": \"https://admarketplace:KIBC07LjHMTTcSdhRzmd@productfeeds.thehut.net/feeds/admarketplace_LFUK_feed.csv\"\n",
    "    },\n",
    "    \"LookFantastic_FR\": {\n",
    "        \"folder\": \"/Volumes/T9/AMP/KlarnaShoppingAds/Look Fantastic/FR\",\n",
    "        \"url\": \"https://admarketplace:KIBC07LjHMTTcSdhRzmd@productfeeds.thehut.net/feeds/admarketplace_LFFR_feed.csv\"\n",
    "    },\n",
    "    \"LookFantastic_IT\": {\n",
    "        \"folder\": \"/Volumes/T9/AMP/KlarnaShoppingAds/Look Fantastic/IT\",\n",
    "        \"url\": \"https://admarketplace:KIBC07LjHMTTcSdhRzmd@productfeeds.thehut.net/feeds/admarketplace_LFIT_feed.csv\"\n",
    "    },\n",
    "    \"NewLook_UK\": {\n",
    "        \"folder\": \"/Volumes/T9/AMP/KlarnaShoppingAds/New Look\",\n",
    "        \"url\": \"https://webadapters.channeladvisor.com/CSEAdapter/Default.aspx?pid=TZPa_C_PAhuxHe*-X%5b%5eI%2f%23QjJJq_%5c%5bU1%5bv%2f%23%5cmBLvb%5cXTK3t%2f%26*iEG%3dk(*P%5bbE1XQnGGs_*%2fX00E%5dX%5b%3drI\"\n",
    "    },\n",
    "    \"Vodafone_UK\": {\n",
    "        \"folder\": \"/Volumes/T9/AMP/KlarnaShoppingAds/Vodafone\",\n",
    "        \"url\": \"https://feed-download.bigupdata.co.uk/download/?lnk=0ef2b49a301a46c29ff0acf5a913ad75\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Columns to remove for Sephora\n",
    "redundant_columns = {\n",
    "    \"c:GA_product_id\", \"c:nov_score\", \"c:profit_margin_flag\", \"c:returns_margin_flag\", \"c:feed_market\",\n",
    "    \"c:allocation\", \"c:stock_level_flag\", \"c:IM_product_name\", \"c:IM_beauty_brand\", \"c:IM_product_category\",\n",
    "    \"c:IM_range\", \"c:flag_plus_size\", \"c:flag_maternity\", \"c:flag_maternity\", \"product_width\", \"product_length\",\n",
    "    \"product_height\", \"shipping_weight\", \"shipping_length\", \"unit_pricing_measure\", \"unit_pricing_base_measure\",\n",
    "    \"product_review_average\", \"c:Main_Highlights\", \"c:Shortened_Name_Ads\", \"c:Alternative Image URL (1)\",\n",
    "    \"c:display ads link\", \"display ads title\", \"adwords_labels\", \"adwords_grouping\", \"c:Was Price (inc VAT)\",\n",
    "    \"c:ultimate_price\", \"promotion_id\", \"subscription_cost\", \"instalment\", \"custom_label_0\", \"custom_label_1\",\n",
    "    \"custom_label_2\", \"custom_label_3\", \"custom_label_4\", \"custom_number_0\", \"custom_number_1\", \"custom_number_2\",\n",
    "    \"custom_number_3\", \"custom_number_4\", \"c:shipping(country:price:min_handling_time:max_han)\", \"shipping_label\",\n",
    "    \"shopping_ads_excluded_country\", \"excluded_destination\", \"return_policy_label\", \"max_handling_time\",\n",
    "    \"min_handling_time\", \"isbn\", \"identifier exists\"\n",
    "}\n",
    "\n",
    "# Generate timestamp for unique filenames\n",
    "current_timestamp = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "# Process each advertiser\n",
    "for advertiser, data in advertisers.items():\n",
    "    folder_path = data[\"folder\"]\n",
    "    url = data[\"url\"]\n",
    "\n",
    "    # Ensure folder exists\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # Determine file extension from URL\n",
    "    extension = \".csv\" if url.endswith(\".csv\") else \".tsv\" if url.endswith(\".tsv\") else \".txt\"\n",
    "    delimiter = \",\" if extension == \".csv\" else \"\\t\"\n",
    "\n",
    "    # Generate input and output file names\n",
    "    input_file_name = f'feed_{current_timestamp}{extension}'\n",
    "    output_file_name = f'{advertiser.lower()}.tsv'  # Save output as .tsv\n",
    "\n",
    "    # Full file paths\n",
    "    input_file_path = os.path.join(folder_path, input_file_name)\n",
    "    output_file_path = os.path.join(folder_path, output_file_name)\n",
    "\n",
    "    # Download the file\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Save the raw downloaded content\n",
    "        with open(input_file_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"Downloaded file for {advertiser} saved as: {input_file_path}\")\n",
    "\n",
    "        # Attempt to read file with correct delimiter\n",
    "        df = pd.read_csv(input_file_path, sep=delimiter, low_memory=False, dtype=str, on_bad_lines=\"skip\")\n",
    "        \n",
    "        # Check if all data is in one column and try alternative delimiter\n",
    "        if len(df.columns) == 1:\n",
    "            print(f\"Warning: {advertiser} data appears to be in one column. Attempting to split...\")\n",
    "            df = pd.read_csv(input_file_path, sep=\"\\t\" if delimiter == \",\" else \",\", low_memory=False, dtype=str, on_bad_lines=\"skip\")\n",
    "        \n",
    "        # Remove redundant columns for Sephora\n",
    "        if advertiser == \"Sephora\":\n",
    "            df = df.drop(columns=[col for col in redundant_columns if col in df.columns], errors='ignore')\n",
    "        \n",
    "        # Save the processed file as TSV\n",
    "        df.to_csv(output_file_path, sep='\\t', index=False)\n",
    "\n",
    "        print(f\"Processed and saved: {output_file_path}\")\n",
    "\n",
    "    except requests.HTTPError as e:\n",
    "        print(f\"HTTP error occurred while downloading {advertiser}: {e}\")\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Error: Could not decode file for {advertiser}. Please check encoding.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error while processing {advertiser}: {e}\")\n",
    "\n",
    "print(\"All downloads complete.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44ffae0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: /Volumes/T9/AMP/KlarnaShoppingAds/Look Fantastic/UK/amp_klarna_lookfantastic_uk.tsv.gz\n",
      "Processed and saved: /Volumes/T9/AMP/KlarnaShoppingAds/Look Fantastic/FR/amp_klarna_lookfantastic_fr.tsv.gz\n",
      "Processed and saved: /Volumes/T9/AMP/KlarnaShoppingAds/Look Fantastic/IT/amp_klarna_lookfantastic_it.tsv.gz\n",
      "Processed and saved: /Volumes/T9/AMP/KlarnaShoppingAds/New Look/amp_klarna_newlook_uk.tsv.gz\n",
      "Processed and saved: /Volumes/T9/AMP/KlarnaShoppingAds/Sephora/amp_klarna_sephora_uk.tsv.gz\n",
      "Processed and saved: /Volumes/T9/AMP/KlarnaShoppingAds/My Protein/UK/amp_klarna_myprotein_uk.tsv.gz\n",
      "Processed and saved: /Volumes/T9/AMP/KlarnaShoppingAds/Vodafone/amp_klarna_vodafone_uk.tsv.gz\n",
      "All advertisers processed successfully.\n",
      "CPU times: user 5.81 s, sys: 283 ms, total: 6.09 s\n",
      "Wall time: 6.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import os\n",
    "\n",
    "# Define advertiser-specific settings\n",
    "advertisers = {\n",
    "    \"LookFantastic_UK\": {\"folder\": \"/Volumes/T9/AMP/KlarnaShoppingAds/Look Fantastic/UK\", \"ctaid\": \"1203\"},\n",
    "    \"LookFantastic_FR\": {\"folder\": \"/Volumes/T9/AMP/KlarnaShoppingAds/Look Fantastic/FR\", \"ctaid\": \"1203\"},\n",
    "    \"LookFantastic_IT\": {\"folder\": \"/Volumes/T9/AMP/KlarnaShoppingAds/Look Fantastic/IT\", \"ctaid\": \"1203\"},\n",
    "    \"NewLook_UK\": {\"folder\": \"/Volumes/T9/AMP/KlarnaShoppingAds/New Look\", \"ctaid\": \"1205\"},\n",
    "    \"Sephora_UK\": {\"folder\": \"/Volumes/T9/AMP/KlarnaShoppingAds/Sephora\", \"ctaid\": \"1325\"},\n",
    "    \"MyProtein_UK\": {\"folder\": \"/Volumes/T9/AMP/KlarnaShoppingAds/My Protein/UK\", \"ctaid\": \"1209\"},\n",
    "    \"Vodafone_UK\": {\"folder\": \"/Volumes/T9/AMP/KlarnaShoppingAds/Vodafone\", \"ctaid\": \"1153\"}\n",
    "}\n",
    "\n",
    "# Standardized column names\n",
    "standard_columns = [\n",
    "    \"SKU/id\", \"Name\", \"Price\", \"Shipping costs\", \"Stock status\", \"Delivery time\", \"Manufacturer\",\n",
    "    \"EAN/GTIN\", \"Manufacturer SKU / MPN\", \"URL\", \"Image URL\", \"Category\", \"Description\", \"Condition\",\n",
    "    \"Sale Price\", \"Sale Price Effective Date\", \"Color\", \"Size\", \"SizeSystem\", \"AdultContent\",\n",
    "    \"AgeGroup\", \"Bundled\", \"EnergyEfficiencyClass\", \"Gender\", \"GroupId\", \"Material\", \"Multipack\", \"Pattern\"\n",
    "]\n",
    "\n",
    "# Column renaming mappings\n",
    "column_mapping_general = {\n",
    "    'id': 'SKU/id', 'title': 'Name', 'price': 'Price', 'shipping': 'Shipping costs', 'availability': 'Stock status',\n",
    "    'availability_date': 'Delivery time', 'brand': 'Manufacturer', 'gtin': 'EAN/GTIN', 'mpn': 'Manufacturer SKU / MPN',\n",
    "    'link': 'URL', 'image_link': 'Image URL', 'google_product_category': 'Category', 'description': 'Description',\n",
    "    'adult': 'AdultContent', 'age_group': 'AgeGroup', 'color': 'Color', 'condition': 'Condition', 'item_group_id': 'GroupId',\n",
    "    'material': 'Material', 'pattern': 'Pattern', 'size': 'Size', 'size_system': 'SizeSystem', 'sale_price': 'Sale Price',\n",
    "    'sale_price_effective_date': 'Sale Price Effective Date', 'gender': 'Gender', 'multipack': 'Multipack',\n",
    "    'bundled': 'Bundled', 'energy_efficiency_class': 'EnergyEfficiencyClass'\n",
    "}\n",
    "\n",
    "column_mapping_myprotein = {\n",
    "    'ID': 'SKU/id', 'Title': 'Name', 'Price': 'Price', 'Exit URL': 'URL', 'Image URL': 'Image URL',\n",
    "    'Availability': 'Stock status', 'Brand': 'Manufacturer', 'gtin': 'EAN/GTIN',\n",
    "    'Google Product Category': 'Category', 'Description': 'Description', 'Size': 'Size'\n",
    "}\n",
    "\n",
    "column_mapping_lookfantastic = {\n",
    "    'id': 'SKU/id', 'name': 'Name', 'price': 'Price', 'producturl': 'URL', 'bigimage': 'Image URL',\n",
    "    'instock': 'Stock status', 'brand': 'Manufacturer', 'gtin': 'EAN/GTIN', 'saleprice': 'Sale Price',\n",
    "    'googlecategory': 'Category', 'Description': 'Description', 'Size': 'Size','gender': 'Gender'\n",
    "}\n",
    "\n",
    "# Base URL template for Klarna tracking\n",
    "base_url_template = \"https://klarnashoppingads.ampxdirect.com/?plid=9z0zxe52a9&ctaid={ctaid}&v=1.3&source=als_tiles\"\n",
    "\n",
    "# Process each advertiser\n",
    "for advertiser, data in advertisers.items():\n",
    "    folder_path = data[\"folder\"]\n",
    "    ctaid = data[\"ctaid\"]\n",
    "    input_file_path = os.path.join(folder_path, f\"{advertiser.lower()}.tsv\")\n",
    "    output_file_path = os.path.join(folder_path, f\"amp_klarna_{advertiser.lower()}.tsv.gz\")\n",
    "\n",
    "    if not os.path.exists(input_file_path):\n",
    "        print(f\"Skipping {advertiser}: File not found ({input_file_path})\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(input_file_path, sep='\\t', low_memory=False, dtype=str, on_bad_lines=\"skip\", quoting=3)\n",
    "    \n",
    "    # LookFantastic-specific fix\n",
    "    if advertiser.startswith(\"LookFantastic\") and \"producturl\" in df.columns: \n",
    "        if \"producturl\" in df.columns:   \n",
    "            df.rename(columns={\"producturl\": \"link\"}, inplace=True)\n",
    "        df.rename(columns=column_mapping_lookfantastic, inplace=True)\n",
    "    \n",
    "    # Apply MyProtein-specific processing\n",
    "    if advertiser == \"MyProtein_UK\":\n",
    "        if \"Exit URL\" in df.columns:\n",
    "            df.rename(columns={\"Exit URL\": \"URL\"}, inplace=True)\n",
    "        df.rename(columns=column_mapping_myprotein, inplace=True)\n",
    "    else:\n",
    "        df.rename(columns=column_mapping_general, inplace=True)\n",
    "    \n",
    "    if 'URL' not in df.columns:\n",
    "        print(f\"Skipping {advertiser}: 'URL' column missing\")\n",
    "        continue\n",
    "\n",
    "    df['URL'] = df['URL'].astype(str).fillna('')\n",
    "    base_url = base_url_template.format(ctaid=ctaid)\n",
    "    df['URL'] = df['URL'].apply(lambda x: f\"{base_url}&cu={urllib.parse.quote_plus(x)}&fbu={urllib.parse.quote_plus(x)}\")\n",
    "\n",
    "    # Clean 'EAN/GTIN'\n",
    "    if 'EAN/GTIN' in df.columns:\n",
    "        df['EAN/GTIN'] = df['EAN/GTIN'].astype(str).apply(lambda x: x.rstrip('.0') if '.0' in x else x)\n",
    "\n",
    "    # Ensure numerical columns retain original values\n",
    "    for col in ['SKU/id', 'EAN/GTIN', 'Price']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).fillna('')\n",
    "\n",
    "    # Add missing columns\n",
    "    for col in standard_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = \"\"\n",
    "\n",
    "    df = df[standard_columns]\n",
    "    df.to_csv(output_file_path, sep='\\t', index=False, compression='gzip')\n",
    "    print(f\"Processed and saved: {output_file_path}\")\n",
    "\n",
    "print(\"All advertisers processed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080bc38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to upload C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Look Fantastic/IT/amp_klarna_lookfantastic_it.tsv.gz: failure: open no such file or directory\n",
      "All processed files uploaded successfully.\n",
      "CPU times: total: 62.5 ms\n",
      "Wall time: 654 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import paramiko\n",
    "import glob\n",
    "\n",
    "# Define advertiser-specific settings\n",
    "advertisers = {\n",
    "    \"LookFantastic_UK\": \"C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Look Fantastic/UK/\",\n",
    "    \"LookFantastic_FR\": \"C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Look Fantastic/FR/\",\n",
    "    \"LookFantastic_IT\": \"C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Look Fantastic/IT/\",\n",
    "    # \"NewLook_UK\": \"C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/New Look\",\n",
    "    \"Sephora_UK\": \"C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Sephora/\",\n",
    "    \"MyProtein_UK\": \"C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/My Protein/UK/\",\n",
    "    \"Vodafone_UK\": \"C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Voda/fone/\",\n",
    "    \"BedBathBeyond_US\": \"C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/BedBathBeyond/\",\n",
    "    \"Bloomingdales_US\": \"C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Bloomingdales/\",\n",
    "    \"HarryDavid_US\": \"C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/HarryDavid/\",\n",
    "    \"Houzz_US\": \"C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Houzz/\",\n",
    "    \"NewBalance_US\": \"C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/NewBalance/\",\n",
    "    \"TheHomeDepot_US\": \"C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/TheHomeDepot/\",\n",
    "    \"TommyBahama_US\": \"C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/TommyBahama/\",\n",
    "    \"Ulta_US\": \"C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Ulta/\",\n",
    "    \"Verizon_US\": \"C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Verizon/\",\n",
    "    \"Wayfair_US\": \"C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Wayfair/\",\n",
    "    \"Zappos_US\": \"C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Zappos\",\n",
    "    \"UnderArmour_US\": \"C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/UnderArmour/\"\n",
    "}\n",
    "\n",
    "# SFTP Configuration\n",
    "sftp_host = \"dev-sftp.admarketplace.net\"\n",
    "sftp_port = 22\n",
    "sftp_username = \"l_klarnapricerun\"\n",
    "sftp_password = \"9ir5nukn2JGEPDC5AZsiett4\"\n",
    "sftp_target_folder = \"/sftp/l_klarnapricerun/files\"\n",
    "\n",
    "# Upload files to SFTP\n",
    "def upload_to_sftp(local_file_path, remote_file_path):\n",
    "    try:\n",
    "        transport = paramiko.Transport((sftp_host, sftp_port))\n",
    "        transport.connect(username=sftp_username, password=sftp_password)\n",
    "        \n",
    "        # Use SFTPClient.from_transport to initialize the SFTP session\n",
    "        with paramiko.SFTPClient.from_transport(transport) as sftp:\n",
    "            sftp.put(local_file_path.replace(\"\\\\\", \"/\"), remote_file_path)\n",
    "            print(f\"Successfully uploaded: {local_file_path} -> {remote_file_path}\")\n",
    "        \n",
    "        transport.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload {local_file_path}: {e}\")\n",
    "\n",
    "# Upload all processed files\n",
    "for advertiser, folder_path in advertisers.items():\n",
    "    file_pattern = os.path.join(folder_path, f\"amp_klarna_{advertiser.lower()}.tsv.gz\").replace(\"\\\\\", \"/\")\n",
    "    matching_files = glob.glob(file_pattern)\n",
    "    \n",
    "    if not matching_files:\n",
    "        print(f\"Skipping {advertiser}: Processed file not found ({file_pattern})\")\n",
    "        continue\n",
    "    \n",
    "    for output_file_path in matching_files:\n",
    "        remote_file_path = os.path.join(sftp_target_folder, os.path.basename(output_file_path)).replace(\"\\\\\", \"/\")\n",
    "        upload_to_sftp(output_file_path, remote_file_path)\n",
    "\n",
    "print(\"All processed files uploaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c492af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Starting test with batch size: 2\n",
      "⏳ Uploading batch: ['LookFantastic_UK', 'LookFantastic_FR']\n",
      "✅ Uploaded: amp_klarna_lookfantastic_uk.tsv.gz\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Look Fantastic/FR/amp_klarna_lookfantastic_fr.tsv.gz: failure: open no such file or directory\n",
      "⏳ Uploading batch: ['LookFantastic_IT', 'Sephora_UK']\n",
      "✅ Uploaded: amp_klarna_lookfantastic_it.tsv.gz\n",
      "✅ Uploaded: amp_klarna_sephora_uk.tsv.gz\n",
      "⏳ Uploading batch: ['MyProtein_UK', 'Vodafone_UK']\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/My Protein/UK/amp_klarna_myprotein_uk.tsv.gz: failure: open no such file or directory\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Vodafone/amp_klarna_vodafone_uk.tsv.gz: failure: open no such file or directory\n",
      "⏳ Uploading batch: ['BedBathBeyond_US', 'Bloomingdales_US']\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/BedBathBeyond/amp_klarna_bedbathbeyond_us.tsv.gz: failure: open no such file or directory\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Bloomingdales/amp_klarna_bloomingdales_us.tsv.gz: failure: open no such file or directory\n",
      "⏳ Uploading batch: ['HarryDavid_US', 'NewBalance_US']\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/HarryDavid/amp_klarna_harrydavid_us.tsv.gz: failure: open no such file or directory\n",
      "✅ Uploaded: amp_klarna_newbalance_us.tsv.gz\n",
      "⏳ Uploading batch: ['TheHomeDepot_US', 'TommyBahama_US']\n",
      "✅ Uploaded: amp_klarna_thehomedepot_us.tsv.gz\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/TommyBahama/amp_klarna_tommybahama_us.tsv.gz: failure: open no such file or directory\n",
      "⏳ Uploading batch: ['Ulta_US', 'Verizon_US']\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Ulta/amp_klarna_ulta_us.tsv.gz: failure: open no such file or directory\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Verizon/amp_klarna_verizon_us.tsv.gz: failure: open no such file or directory\n",
      "⏳ Uploading batch: ['Zappos_US', 'UnderArmour_US']\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Zappos/amp_klarna_zappos_us.tsv.gz: failure: open no such file or directory\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/UnderArmour/amp_klarna_underarmour_us.tsv.gz: failure: open no such file or directory\n",
      "✅ Finished round for batch size 2\n",
      "\n",
      "🚀 Starting test with batch size: 3\n",
      "⏳ Uploading batch: ['LookFantastic_UK', 'LookFantastic_FR', 'LookFantastic_IT']\n",
      "✅ Uploaded: amp_klarna_lookfantastic_uk.tsv.gz\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Look Fantastic/FR/amp_klarna_lookfantastic_fr.tsv.gz: failure: open no such file or directory\n",
      "✅ Uploaded: amp_klarna_lookfantastic_it.tsv.gz\n",
      "⏳ Uploading batch: ['Sephora_UK', 'MyProtein_UK', 'Vodafone_UK']\n",
      "✅ Uploaded: amp_klarna_sephora_uk.tsv.gz\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/My Protein/UK/amp_klarna_myprotein_uk.tsv.gz: failure: open no such file or directory\n",
      "✅ Uploaded: amp_klarna_vodafone_uk.tsv.gz\n",
      "⏳ Uploading batch: ['BedBathBeyond_US', 'Bloomingdales_US', 'HarryDavid_US']\n",
      "✅ Uploaded: amp_klarna_bedbathbeyond_us.tsv.gz\n",
      "✅ Uploaded: amp_klarna_bloomingdales_us.tsv.gz\n",
      "✅ Uploaded: amp_klarna_harrydavid_us.tsv.gz\n",
      "⏳ Uploading batch: ['NewBalance_US', 'TheHomeDepot_US', 'TommyBahama_US']\n",
      "✅ Uploaded: amp_klarna_newbalance_us.tsv.gz\n",
      "✅ Uploaded: amp_klarna_thehomedepot_us.tsv.gz\n",
      "✅ Uploaded: amp_klarna_tommybahama_us.tsv.gz\n",
      "⏳ Uploading batch: ['Ulta_US', 'Verizon_US', 'Zappos_US']\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Ulta/amp_klarna_ulta_us.tsv.gz: failure: open no such file or directory\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Verizon/amp_klarna_verizon_us.tsv.gz: failure: open no such file or directory\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Zappos/amp_klarna_zappos_us.tsv.gz: failure: open no such file or directory\n",
      "⏳ Uploading batch: ['UnderArmour_US']\n",
      "✅ Uploaded: amp_klarna_underarmour_us.tsv.gz\n",
      "✅ Finished round for batch size 3\n",
      "\n",
      "🚀 Starting test with batch size: 4\n",
      "⏳ Uploading batch: ['LookFantastic_UK', 'LookFantastic_FR', 'LookFantastic_IT', 'Sephora_UK']\n",
      "✅ Uploaded: amp_klarna_lookfantastic_uk.tsv.gz\n",
      "✅ Uploaded: amp_klarna_lookfantastic_fr.tsv.gz\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Look Fantastic/IT/amp_klarna_lookfantastic_it.tsv.gz: failure: open no such file or directory\n",
      "✅ Uploaded: amp_klarna_sephora_uk.tsv.gz\n",
      "⏳ Uploading batch: ['MyProtein_UK', 'Vodafone_UK', 'BedBathBeyond_US', 'Bloomingdales_US']\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/My Protein/UK/amp_klarna_myprotein_uk.tsv.gz: failure: open no such file or directory\n",
      "✅ Uploaded: amp_klarna_vodafone_uk.tsv.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Socket exception: An existing connection was forcibly closed by the remote host (10054)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/BedBathBeyond/amp_klarna_bedbathbeyond_us.tsv.gz: \n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Bloomingdales/amp_klarna_bloomingdales_us.tsv.gz: failure: open no such file or directory\n",
      "⏳ Uploading batch: ['HarryDavid_US', 'NewBalance_US', 'TheHomeDepot_US', 'TommyBahama_US']\n",
      "✅ Uploaded: amp_klarna_harrydavid_us.tsv.gz\n",
      "✅ Uploaded: amp_klarna_newbalance_us.tsv.gz\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/TheHomeDepot/amp_klarna_thehomedepot_us.tsv.gz: failure: open no such file or directory\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/TommyBahama/amp_klarna_tommybahama_us.tsv.gz: failure: open no such file or directory\n",
      "⏳ Uploading batch: ['Ulta_US', 'Verizon_US', 'Zappos_US', 'UnderArmour_US']\n",
      "✅ Uploaded: amp_klarna_ulta_us.tsv.gz\n",
      "✅ Uploaded: amp_klarna_verizon_us.tsv.gz\n",
      "✅ Uploaded: amp_klarna_zappos_us.tsv.gz\n",
      "✅ Uploaded: amp_klarna_underarmour_us.tsv.gz\n",
      "✅ Finished round for batch size 4\n",
      "\n",
      "🚀 Starting test with batch size: 5\n",
      "⏳ Uploading batch: ['LookFantastic_UK', 'LookFantastic_FR', 'LookFantastic_IT', 'Sephora_UK', 'MyProtein_UK']\n",
      "✅ Uploaded: amp_klarna_lookfantastic_uk.tsv.gz\n",
      "✅ Uploaded: amp_klarna_lookfantastic_fr.tsv.gz\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Look Fantastic/IT/amp_klarna_lookfantastic_it.tsv.gz: failure: open no such file or directory\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Sephora/amp_klarna_sephora_uk.tsv.gz: failure: open no such file or directory\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/My Protein/UK/amp_klarna_myprotein_uk.tsv.gz: failure: open no such file or directory\n",
      "⏳ Uploading batch: ['Vodafone_UK', 'BedBathBeyond_US', 'Bloomingdales_US', 'HarryDavid_US', 'NewBalance_US']\n",
      "✅ Uploaded: amp_klarna_vodafone_uk.tsv.gz\n",
      "✅ Uploaded: amp_klarna_bedbathbeyond_us.tsv.gz\n",
      "✅ Uploaded: amp_klarna_bloomingdales_us.tsv.gz\n",
      "✅ Uploaded: amp_klarna_harrydavid_us.tsv.gz\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/NewBalance/amp_klarna_newbalance_us.tsv.gz: failure: open no such file or directory\n",
      "⏳ Uploading batch: ['TheHomeDepot_US', 'TommyBahama_US', 'Ulta_US', 'Verizon_US', 'Zappos_US']\n",
      "✅ Uploaded: amp_klarna_thehomedepot_us.tsv.gz\n",
      "✅ Uploaded: amp_klarna_tommybahama_us.tsv.gz\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Ulta/amp_klarna_ulta_us.tsv.gz: failure: open no such file or directory\n",
      "✅ Uploaded: amp_klarna_verizon_us.tsv.gz\n",
      "✅ Uploaded: amp_klarna_zappos_us.tsv.gz\n",
      "⏳ Uploading batch: ['UnderArmour_US']\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/UnderArmour/amp_klarna_underarmour_us.tsv.gz: failure: open no such file or directory\n",
      "✅ Finished round for batch size 5\n",
      "\n",
      "🚀 Starting test with batch size: 6\n",
      "⏳ Uploading batch: ['LookFantastic_UK', 'LookFantastic_FR', 'LookFantastic_IT', 'Sephora_UK', 'MyProtein_UK', 'Vodafone_UK']\n",
      "✅ Uploaded: amp_klarna_lookfantastic_uk.tsv.gz\n",
      "✅ Uploaded: amp_klarna_lookfantastic_fr.tsv.gz\n",
      "✅ Uploaded: amp_klarna_lookfantastic_it.tsv.gz\n",
      "✅ Uploaded: amp_klarna_sephora_uk.tsv.gz\n",
      "✅ Uploaded: amp_klarna_myprotein_uk.tsv.gz\n",
      "✅ Uploaded: amp_klarna_vodafone_uk.tsv.gz\n",
      "⏳ Uploading batch: ['BedBathBeyond_US', 'Bloomingdales_US', 'HarryDavid_US', 'NewBalance_US', 'TheHomeDepot_US', 'TommyBahama_US']\n",
      "✅ Uploaded: amp_klarna_bedbathbeyond_us.tsv.gz\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Bloomingdales/amp_klarna_bloomingdales_us.tsv.gz: failure: open no such file or directory\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/HarryDavid/amp_klarna_harrydavid_us.tsv.gz: failure: open no such file or directory\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/NewBalance/amp_klarna_newbalance_us.tsv.gz: failure: open no such file or directory\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/TheHomeDepot/amp_klarna_thehomedepot_us.tsv.gz: failure: open no such file or directory\n",
      "✅ Uploaded: amp_klarna_tommybahama_us.tsv.gz\n",
      "⏳ Uploading batch: ['Ulta_US', 'Verizon_US', 'Zappos_US', 'UnderArmour_US']\n",
      "✅ Uploaded: amp_klarna_ulta_us.tsv.gz\n",
      "✅ Uploaded: amp_klarna_verizon_us.tsv.gz\n",
      "✅ Uploaded: amp_klarna_zappos_us.tsv.gz\n",
      "✅ Uploaded: amp_klarna_underarmour_us.tsv.gz\n",
      "✅ Finished round for batch size 6\n",
      "\n",
      "🚀 Starting test with batch size: 7\n",
      "⏳ Uploading batch: ['LookFantastic_UK', 'LookFantastic_FR', 'LookFantastic_IT', 'Sephora_UK', 'MyProtein_UK', 'Vodafone_UK', 'BedBathBeyond_US']\n",
      "✅ Uploaded: amp_klarna_lookfantastic_uk.tsv.gz\n",
      "✅ Uploaded: amp_klarna_lookfantastic_fr.tsv.gz\n",
      "✅ Uploaded: amp_klarna_lookfantastic_it.tsv.gz\n",
      "✅ Uploaded: amp_klarna_sephora_uk.tsv.gz\n",
      "✅ Uploaded: amp_klarna_myprotein_uk.tsv.gz\n",
      "✅ Uploaded: amp_klarna_vodafone_uk.tsv.gz\n",
      "✅ Uploaded: amp_klarna_bedbathbeyond_us.tsv.gz\n",
      "⏳ Uploading batch: ['Bloomingdales_US', 'HarryDavid_US', 'NewBalance_US', 'TheHomeDepot_US', 'TommyBahama_US', 'Ulta_US', 'Verizon_US']\n",
      "✅ Uploaded: amp_klarna_bloomingdales_us.tsv.gz\n",
      "✅ Uploaded: amp_klarna_harrydavid_us.tsv.gz\n",
      "✅ Uploaded: amp_klarna_newbalance_us.tsv.gz\n",
      "✅ Uploaded: amp_klarna_thehomedepot_us.tsv.gz\n",
      "✅ Uploaded: amp_klarna_tommybahama_us.tsv.gz\n",
      "✅ Uploaded: amp_klarna_ulta_us.tsv.gz\n",
      "✅ Uploaded: amp_klarna_verizon_us.tsv.gz\n",
      "⏳ Uploading batch: ['Zappos_US', 'UnderArmour_US']\n",
      "✅ Uploaded: amp_klarna_zappos_us.tsv.gz\n",
      "✅ Uploaded: amp_klarna_underarmour_us.tsv.gz\n",
      "✅ Finished round for batch size 7\n",
      "\n",
      "🚀 Starting test with batch size: 8\n",
      "⏳ Uploading batch: ['LookFantastic_UK', 'LookFantastic_FR', 'LookFantastic_IT', 'Sephora_UK', 'MyProtein_UK', 'Vodafone_UK', 'BedBathBeyond_US', 'Bloomingdales_US']\n",
      "✅ Uploaded: amp_klarna_lookfantastic_uk.tsv.gz\n",
      "✅ Uploaded: amp_klarna_lookfantastic_fr.tsv.gz\n",
      "✅ Uploaded: amp_klarna_lookfantastic_it.tsv.gz\n",
      "✅ Uploaded: amp_klarna_sephora_uk.tsv.gz\n",
      "✅ Uploaded: amp_klarna_myprotein_uk.tsv.gz\n",
      "✅ Uploaded: amp_klarna_vodafone_uk.tsv.gz\n",
      "✅ Uploaded: amp_klarna_bedbathbeyond_us.tsv.gz\n",
      "✅ Uploaded: amp_klarna_bloomingdales_us.tsv.gz\n",
      "⏳ Uploading batch: ['HarryDavid_US', 'NewBalance_US', 'TheHomeDepot_US', 'TommyBahama_US', 'Ulta_US', 'Verizon_US', 'Zappos_US', 'UnderArmour_US']\n",
      "✅ Uploaded: amp_klarna_harrydavid_us.tsv.gz\n",
      "✅ Uploaded: amp_klarna_newbalance_us.tsv.gz\n",
      "✅ Uploaded: amp_klarna_thehomedepot_us.tsv.gz\n",
      "✅ Uploaded: amp_klarna_tommybahama_us.tsv.gz\n",
      "✅ Uploaded: amp_klarna_ulta_us.tsv.gz\n",
      "✅ Uploaded: amp_klarna_verizon_us.tsv.gz\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:66\u001b[0m\n",
      "File \u001b[1;32m<timed exec>:41\u001b[0m, in \u001b[0;36mupload_to_sftp\u001b[1;34m(local_file_path, remote_file_path)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ywang\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\paramiko\\sftp_client.py:759\u001b[0m, in \u001b[0;36mSFTPClient.put\u001b[1;34m(self, localpath, remotepath, callback, confirm)\u001b[0m\n\u001b[0;32m    757\u001b[0m file_size \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstat(localpath)\u001b[38;5;241m.\u001b[39mst_size\n\u001b[0;32m    758\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(localpath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fl:\n\u001b[1;32m--> 759\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mputfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremotepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfirm\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ywang\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\paramiko\\sftp_client.py:716\u001b[0m, in \u001b[0;36mSFTPClient.putfo\u001b[1;34m(self, fl, remotepath, file_size, callback, confirm)\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile(remotepath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fr:\n\u001b[0;32m    715\u001b[0m     fr\u001b[38;5;241m.\u001b[39mset_pipelined(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 716\u001b[0m     size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transfer_with_callback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m confirm:\n\u001b[0;32m    720\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstat(remotepath)\n",
      "File \u001b[1;32mc:\\Users\\ywang\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\paramiko\\sftp_client.py:679\u001b[0m, in \u001b[0;36mSFTPClient._transfer_with_callback\u001b[1;34m(self, reader, writer, file_size, callback)\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    678\u001b[0m     data \u001b[38;5;241m=\u001b[39m reader\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m32768\u001b[39m)\n\u001b[1;32m--> 679\u001b[0m     \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    680\u001b[0m     size \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[0;32m    681\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ywang\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\paramiko\\file.py:389\u001b[0m, in \u001b[0;36mBufferedFile.write\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not open for writing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flags \u001b[38;5;241m&\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mFLAG_BUFFERED):\n\u001b[1;32m--> 389\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wbuffer\u001b[38;5;241m.\u001b[39mwrite(data)\n",
      "File \u001b[1;32mc:\\Users\\ywang\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\paramiko\\file.py:507\u001b[0m, in \u001b[0;36mBufferedFile._write_all\u001b[1;34m(self, raw_data)\u001b[0m\n\u001b[0;32m    505\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmemoryview\u001b[39m(raw_data)\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 507\u001b[0m     count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m     data \u001b[38;5;241m=\u001b[39m data[count:]\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flags \u001b[38;5;241m&\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mFLAG_APPEND:\n",
      "File \u001b[1;32mc:\\Users\\ywang\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\paramiko\\sftp_file.py:195\u001b[0m, in \u001b[0;36mSFTPFile._write\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_write\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;66;03m# may write less than requested if it would exceed max packet size\u001b[39;00m\n\u001b[0;32m    194\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_REQUEST_SIZE)\n\u001b[1;32m--> 195\u001b[0m     sftp_async_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msftp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_async_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCMD_WRITE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mint64\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_realpos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reqs\u001b[38;5;241m.\u001b[39mappend(sftp_async_request)\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipelined \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m    204\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reqs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msftp\u001b[38;5;241m.\u001b[39msock\u001b[38;5;241m.\u001b[39mrecv_ready()\n\u001b[0;32m    205\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\ywang\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\paramiko\\sftp_client.py:881\u001b[0m, in \u001b[0;36mSFTPClient._async_request\u001b[1;34m(self, fileobj, t, *args)\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m--> 881\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_packet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m num\n",
      "File \u001b[1;32mc:\\Users\\ywang\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\paramiko\\sftp.py:209\u001b[0m, in \u001b[0;36mBaseSFTP._send_packet\u001b[1;34m(self, t, packet)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39multra_debug:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log(DEBUG, util\u001b[38;5;241m.\u001b[39mformat_binary(out, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOUT: \u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m--> 209\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ywang\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\paramiko\\sftp.py:173\u001b[0m, in \u001b[0;36mBaseSFTP._write_all\u001b[1;34m(self, out)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_write_all\u001b[39m(\u001b[38;5;28mself\u001b[39m, out):\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 173\u001b[0m         n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    175\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\Users\\ywang\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\paramiko\\channel.py:799\u001b[0m, in \u001b[0;36mChannel.send\u001b[1;34m(self, s)\u001b[0m\n\u001b[0;32m    797\u001b[0m m\u001b[38;5;241m.\u001b[39madd_byte(cMSG_CHANNEL_DATA)\n\u001b[0;32m    798\u001b[0m m\u001b[38;5;241m.\u001b[39madd_int(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremote_chanid)\n\u001b[1;32m--> 799\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ywang\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\paramiko\\channel.py:1206\u001b[0m, in \u001b[0;36mChannel._send\u001b[1;34m(self, s, m)\u001b[0m\n\u001b[0;32m   1203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m   1204\u001b[0m \u001b[38;5;66;03m# Note: We release self.lock before calling _send_user_message.\u001b[39;00m\n\u001b[0;32m   1205\u001b[0m \u001b[38;5;66;03m# Otherwise, we can deadlock during re-keying.\u001b[39;00m\n\u001b[1;32m-> 1206\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_user_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m size\n",
      "File \u001b[1;32mc:\\Users\\ywang\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\paramiko\\transport.py:1988\u001b[0m, in \u001b[0;36mTransport._send_user_message\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1984\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSHException(\n\u001b[0;32m   1985\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey-exchange timed out waiting for key negotiation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1986\u001b[0m         )  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m   1987\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1988\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1989\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1990\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclear_to_send_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\ywang\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\paramiko\\transport.py:1964\u001b[0m, in \u001b[0;36mTransport._send_message\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1963\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_send_message\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[1;32m-> 1964\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpacketizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ywang\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\paramiko\\packet.py:468\u001b[0m, in \u001b[0;36mPacketizer.send_message\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SSHException(\n\u001b[0;32m    465\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSequence number rolled over during initial kex!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    466\u001b[0m     )\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__sequence_number_out \u001b[38;5;241m=\u001b[39m next_seq\n\u001b[1;32m--> 468\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__sent_bytes \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(out)\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__sent_packets \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ywang\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\paramiko\\packet.py:354\u001b[0m, in \u001b[0;36mPacketizer.write_all\u001b[1;34m(self, out)\u001b[0m\n\u001b[0;32m    352\u001b[0m retry_write \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 354\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mtimeout:\n\u001b[0;32m    356\u001b[0m     retry_write \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import paramiko\n",
    "\n",
    "# Advertiser folders\n",
    "advertisers = {\n",
    "    \"LookFantastic_UK\": \"C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Look Fantastic/UK/\",\n",
    "    \"LookFantastic_FR\": \"C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Look Fantastic/FR/\",\n",
    "    \"LookFantastic_IT\": \"C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Look Fantastic/IT/\",\n",
    "    \"Sephora_UK\": \"C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Sephora/\",\n",
    "    \"MyProtein_UK\": \"C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/My Protein/UK/\",\n",
    "    \"Vodafone_UK\": \"C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Vodafone/\",\n",
    "    \"BedBathBeyond_US\": \"C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/BedBathBeyond/\",\n",
    "    \"Bloomingdales_US\": \"C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Bloomingdales/\",\n",
    "    \"HarryDavid_US\": \"C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/HarryDavid/\",\n",
    "    \"Houzz_US\": \"C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Houzz/\",\n",
    "    \"NewBalance_US\": \"C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/NewBalance/\",\n",
    "    \"TheHomeDepot_US\": \"C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/TheHomeDepot/\",\n",
    "    \"TommyBahama_US\": \"C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/TommyBahama/\",\n",
    "    \"Ulta_US\": \"C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Ulta/\",\n",
    "    \"Verizon_US\": \"C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Verizon/\",\n",
    "    \"Wayfair_US\": \"C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Wayfair/\",\n",
    "    \"Zappos_US\": \"C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/Zappos\",\n",
    "    \"UnderArmour_US\": \"C:/Users/ywang/Documents/Codes/Shopping_Ads/Klarna/UnderArmour/\"\n",
    "}\n",
    "\n",
    "# SFTP credentials\n",
    "sftp_host = \"dev-sftp.admarketplace.net\"\n",
    "sftp_port = 22\n",
    "sftp_username = \"l_klarnapricerun\"\n",
    "sftp_password = \"9ir5nukn2JGEPDC5AZsiett4\"\n",
    "sftp_target_folder = \"/sftp/l_klarnapricerun/files\"\n",
    "\n",
    "# Upload function\n",
    "def upload_to_sftp(local_file_path, remote_file_path):\n",
    "    try:\n",
    "        transport = paramiko.Transport((sftp_host, sftp_port))\n",
    "        transport.connect(username=sftp_username, password=sftp_password)\n",
    "        with paramiko.SFTPClient.from_transport(transport) as sftp:\n",
    "            sftp.put(local_file_path.replace(\"\\\\\", \"/\"), remote_file_path)\n",
    "            print(f\"✅ Uploaded: {os.path.basename(local_file_path)}\")\n",
    "        transport.close()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to upload {local_file_path}: {e}\")\n",
    "\n",
    "# Gather all uploadable files\n",
    "upload_tasks = []\n",
    "for advertiser, folder_path in advertisers.items():\n",
    "    expected_filename = f\"amp_klarna_{advertiser.lower()}.tsv.gz\"\n",
    "    file_path = os.path.join(folder_path, expected_filename).replace(\"\\\\\", \"/\")\n",
    "    if os.path.exists(file_path):\n",
    "        remote_path = os.path.join(sftp_target_folder, os.path.basename(file_path)).replace(\"\\\\\", \"/\")\n",
    "        upload_tasks.append((advertiser, file_path, remote_path))\n",
    "    else:\n",
    "        print(f\"⚠️ Skipping {advertiser} — file not found: {file_path}\")\n",
    "\n",
    "# Try different batch sizes\n",
    "for batch_size in range(2, len(upload_tasks) + 1):\n",
    "    print(f\"\\n🚀 Starting test with batch size: {batch_size}\")\n",
    "    for i in range(0, len(upload_tasks), batch_size):\n",
    "        batch = upload_tasks[i:i + batch_size]\n",
    "        print(f\"⏳ Uploading batch: {[adv for adv, _, _ in batch]}\")\n",
    "\n",
    "        for adv, local_file, remote_file in batch:\n",
    "            upload_to_sftp(local_file, remote_file)\n",
    "\n",
    "        time.sleep(1)  # 1 second delay between batches\n",
    "    print(f\"✅ Finished round for batch size {batch_size}\")\n",
    "\n",
    "print(\"\\n🎉 All batch test uploads complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "813378dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload, LookFantastic_UK, C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/Look Fantastic/UK/amp_klarna_lookfantastic_uk.tsv.gz,/files/amp_klarna_lookfantastic_uk.tsv.gz\n",
      "✅ Uploaded: amp_klarna_lookfantastic_uk.tsv.gz\n",
      "upload, LookFantastic_FR, C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/Look Fantastic/FR/amp_klarna_lookfantastic_fr.tsv.gz,/files/amp_klarna_lookfantastic_fr.tsv.gz\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/Look Fantastic/FR/amp_klarna_lookfantastic_fr.tsv.gz: failure: open no such file or directory\n",
      "upload, LookFantastic_IT, C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/Look Fantastic/IT/amp_klarna_lookfantastic_it.tsv.gz,/files/amp_klarna_lookfantastic_it.tsv.gz\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/Look Fantastic/IT/amp_klarna_lookfantastic_it.tsv.gz: failure: open no such file or directory\n",
      "upload, Sephora_UK, C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/Sephora/amp_klarna_sephora_uk.tsv.gz,/files/amp_klarna_sephora_uk.tsv.gz\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/Sephora/amp_klarna_sephora_uk.tsv.gz: failure: open no such file or directory\n",
      "upload, MyProtein_UK, C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/My Protein/UK/amp_klarna_myprotein_uk.tsv.gz,/files/amp_klarna_myprotein_uk.tsv.gz\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/My Protein/UK/amp_klarna_myprotein_uk.tsv.gz: failure: open no such file or directory\n",
      "upload, Vodafone_UK, C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/Vodafone/amp_klarna_vodafone_uk.tsv.gz,/files/amp_klarna_vodafone_uk.tsv.gz\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/Vodafone/amp_klarna_vodafone_uk.tsv.gz: failure: open no such file or directory\n",
      "upload, BedBathBeyond_US, C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/BedBathBeyond/amp_klarna_bedbathbeyond_us.tsv.gz,/files/amp_klarna_bedbathbeyond_us.tsv.gz\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/BedBathBeyond/amp_klarna_bedbathbeyond_us.tsv.gz: failure: open no such file or directory\n",
      "upload, Bloomingdales_US, C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/Bloomingdales/amp_klarna_bloomingdales_us.tsv.gz,/files/amp_klarna_bloomingdales_us.tsv.gz\n",
      "✅ Uploaded: amp_klarna_bloomingdales_us.tsv.gz\n",
      "upload, HarryDavid_US, C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/HarryDavid/amp_klarna_harrydavid_us.tsv.gz,/files/amp_klarna_harrydavid_us.tsv.gz\n",
      "✅ Uploaded: amp_klarna_harrydavid_us.tsv.gz\n",
      "upload, Houzz_US, C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/Houzz/amp_klarna_houzz_us.tsv.gz,/files/amp_klarna_houzz_us.tsv.gz\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/Houzz/amp_klarna_houzz_us.tsv.gz: failure: open no such file or directory\n",
      "upload, NewBalance_US, C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/NewBalance/amp_klarna_newbalance_us.tsv.gz,/files/amp_klarna_newbalance_us.tsv.gz\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/NewBalance/amp_klarna_newbalance_us.tsv.gz: failure: open no such file or directory\n",
      "upload, TheHomeDepot_US, C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/TheHomeDepot/amp_klarna_thehomedepot_us.tsv.gz,/files/amp_klarna_thehomedepot_us.tsv.gz\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/TheHomeDepot/amp_klarna_thehomedepot_us.tsv.gz: failure: open no such file or directory\n",
      "upload, TommyBahama_US, C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/TommyBahama/amp_klarna_tommybahama_us.tsv.gz,/files/amp_klarna_tommybahama_us.tsv.gz\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/TommyBahama/amp_klarna_tommybahama_us.tsv.gz: failure: open no such file or directory\n",
      "upload, Ulta_US, C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/Ulta/amp_klarna_ulta_us.tsv.gz,/files/amp_klarna_ulta_us.tsv.gz\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/Ulta/amp_klarna_ulta_us.tsv.gz: failure: open no such file or directory\n",
      "upload, Verizon_US, C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/Verizon/amp_klarna_verizon_us.tsv.gz,/files/amp_klarna_verizon_us.tsv.gz\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/Verizon/amp_klarna_verizon_us.tsv.gz: failure: open no such file or directory\n",
      "upload, Wayfair_US, C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/Wayfair/amp_klarna_wayfair_us.tsv.gz,/files/amp_klarna_wayfair_us.tsv.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Socket exception: An existing connection was forcibly closed by the remote host (10054)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/Wayfair/amp_klarna_wayfair_us.tsv.gz: \n",
      "upload, Zappos_US, C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/Zappos/amp_klarna_zappos_us.tsv.gz,/files/amp_klarna_zappos_us.tsv.gz\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/Zappos/amp_klarna_zappos_us.tsv.gz: failure: open no such file or directory\n",
      "upload, UnderArmour_US, C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/UnderArmour/amp_klarna_underarmour_us.tsv.gz,/files/amp_klarna_underarmour_us.tsv.gz\n",
      "❌ Failed to upload C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/UnderArmour/amp_klarna_underarmour_us.tsv.gz: failure: open no such file or directory\n",
      "\n",
      "🎉 All batch test uploads complete.\n",
      "CPU times: total: 40.8 s\n",
      "Wall time: 3min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import paramiko\n",
    "\n",
    "advertisers = {\n",
    "    \"LookFantastic_UK\": \"/Volumes/T9/AMP/KlarnaShoppingAds/Look Fantastic/UK/\",\n",
    "    \"LookFantastic_FR\": \"/Volumes/T9/AMP/KlarnaShoppingAds/Look Fantastic/FR/\",\n",
    "    \"LookFantastic_IT\": \"/Volumes/T9/AMP/KlarnaShoppingAds/Look Fantastic/IT/\",\n",
    "    \"Sephora_UK\": \"/Volumes/T9/AMP/KlarnaShoppingAds/Sephora/\",\n",
    "    \"MyProtein_UK\": \"/Volumes/T9/AMP/KlarnaShoppingAds/My Protein/UK/\",\n",
    "    \"Vodafone_UK\": \"/Volumes/T9/AMP/KlarnaShoppingAds/Vodafone/\",\n",
    "    \"BedBathBeyond_US\": \"/Volumes/T9/AMP/KlarnaShoppingAds/BedBathBeyond/\",\n",
    "    \"Bloomingdales_US\": \"/Volumes/T9/AMP/KlarnaShoppingAds/Bloomingdales/\",\n",
    "    \"HarryDavid_US\": \"/Volumes/T9/AMP/KlarnaShoppingAds/HarryDavid/\",\n",
    "    \"Houzz_US\": \"/Volumes/T9/AMP/KlarnaShoppingAds/Houzz/\",\n",
    "    \"NewBalance_US\": \"/Volumes/T9/AMP/KlarnaShoppingAds/NewBalance/\",\n",
    "    \"TheHomeDepot_US\": \"/Volumes/T9/AMP/KlarnaShoppingAds/TheHomeDepot/\",\n",
    "    \"TommyBahama_US\": \"/Volumes/T9/AMP/KlarnaShoppingAds/TommyBahama/\",\n",
    "    \"Ulta_US\": \"/Volumes/T9/AMP/KlarnaShoppingAds/Ulta/\",\n",
    "    \"Verizon_US\": \"/Volumes/T9/AMP/KlarnaShoppingAds/Verizon/\",\n",
    "    \"Wayfair_US\": \"/Volumes/T9/AMP/KlarnaShoppingAds/Wayfair/\",\n",
    "    \"Zappos_US\": \"/Volumes/T9/AMP/KlarnaShoppingAds/Zappos\",\n",
    "    \"UnderArmour_US\": \"/Volumes/T9/AMP/KlarnaShoppingAds/UnderArmour/\"\n",
    "}\n",
    "\n",
    "sftp_host = \"dev-sftp.admarketplace.net\"\n",
    "sftp_port = 22\n",
    "sftp_username = \"l_klarnapricerun\"\n",
    "sftp_password = \"9ir5nukn2JGEPDC5AZsiett4\"\n",
    "sftp_target_folder = \"/files\"\n",
    "\n",
    "def upload_to_sftp(local_file_path, remote_file_path):\n",
    "    transport = None\n",
    "    try:\n",
    "        transport = paramiko.Transport((sftp_host, sftp_port))\n",
    "        transport.connect(username=sftp_username, password=sftp_password)\n",
    "        with paramiko.SFTPClient.from_transport(transport) as sftp:\n",
    "            sftp.put(local_file_path.replace(\"\\\\\", \"/\"), remote_file_path)\n",
    "            print(f\"✅ Uploaded: {os.path.basename(local_file_path)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to upload {local_file_path}: {e}\")\n",
    "    finally:\n",
    "        if transport and transport.is_active():\n",
    "            transport.close()\n",
    "\n",
    "upload_tasks = []\n",
    "for advertiser, folder_path in advertisers.items():\n",
    "    expected_filename = f\"amp_klarna_{advertiser.lower()}.tsv.gz\"\n",
    "    file_path = os.path.join(folder_path, expected_filename).replace(\"\\\\\", \"/\")\n",
    "    if os.path.exists(file_path):\n",
    "        remote_path = os.path.join(sftp_target_folder, os.path.basename(file_path)).replace(\"\\\\\", \"/\")\n",
    "        upload_tasks.append((advertiser, file_path, remote_path))\n",
    "    else:\n",
    "        print(f\"⚠️ Skipping {advertiser} — file not found: {file_path}\")\n",
    "\n",
    "\n",
    "for tasks in upload_tasks:\n",
    "    print(f\"upload, {tasks[0]}, {tasks[1]},{tasks[2]}\")\n",
    "    upload_to_sftp(tasks[1], tasks[2])\n",
    "\n",
    "\n",
    "# for batch_size in range(2, len(upload_tasks) + 1):\n",
    "#     print(f\"\\n🚀 Starting test with batch size: {batch_size}\")\n",
    "#     for i in range(0, len(upload_tasks), batch_size):\n",
    "#         batch = upload_tasks[i:i + batch_size]\n",
    "#         print(f\"⏳ Uploading batch: {[adv for adv, _, _ in batch]}\")\n",
    "\n",
    "#         for adv, local_file, remote_file in batch:\n",
    "#             upload_to_sftp(local_file, remote_file)\n",
    "\n",
    "#         # time.sleep(1)  # Delay between batches\n",
    "#     print(f\"✅ Finished round for batch size {batch_size}\")\n",
    "\n",
    "print(\"\\n🎉 All batch test uploads complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c6ada6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Searching for: C:\\Users\\ywang\\Documents\\Codes\\Shopping Ads\\Klarna\\Look Fantastic\\UK\\amp_klarna_lookfantastic_uk.tsv.gz\n",
      "✅ Uploaded: C:\\Users\\ywang\\Documents\\Codes\\Shopping Ads\\Klarna\\Look Fantastic\\UK\\amp_klarna_lookfantastic_uk.tsv.gz -> /files/amp_klarna_lookfantastic_uk.tsv.gz\n",
      "🔍 Searching for: C:\\Users\\ywang\\Documents\\Codes\\Shopping Ads\\Klarna\\Look Fantastic\\FR\\amp_klarna_lookfantastic_fr.tsv.gz\n",
      "✅ Uploaded: C:\\Users\\ywang\\Documents\\Codes\\Shopping Ads\\Klarna\\Look Fantastic\\FR\\amp_klarna_lookfantastic_fr.tsv.gz -> /files/amp_klarna_lookfantastic_fr.tsv.gz\n",
      "🔍 Searching for: C:\\Users\\ywang\\Documents\\Codes\\Shopping Ads\\Klarna\\Look Fantastic\\IT\\amp_klarna_lookfantastic_it.tsv.gz\n",
      "✅ Uploaded: C:\\Users\\ywang\\Documents\\Codes\\Shopping Ads\\Klarna\\Look Fantastic\\IT\\amp_klarna_lookfantastic_it.tsv.gz -> /files/amp_klarna_lookfantastic_it.tsv.gz\n",
      "🔍 Searching for: C:\\Users\\ywang\\Documents\\Codes\\Shopping Ads\\Klarna\\Sephora\\amp_klarna_sephora_uk.tsv.gz\n",
      "✅ Uploaded: C:\\Users\\ywang\\Documents\\Codes\\Shopping Ads\\Klarna\\Sephora\\amp_klarna_sephora_uk.tsv.gz -> /files/amp_klarna_sephora_uk.tsv.gz\n",
      "🔍 Searching for: C:\\Users\\ywang\\Documents\\Codes\\Shopping Ads\\Klarna\\My Protein\\UK\\amp_klarna_myprotein_uk.tsv.gz\n",
      "✅ Uploaded: C:\\Users\\ywang\\Documents\\Codes\\Shopping Ads\\Klarna\\My Protein\\UK\\amp_klarna_myprotein_uk.tsv.gz -> /files/amp_klarna_myprotein_uk.tsv.gz\n",
      "🔍 Searching for: C:\\Users\\ywang\\Documents\\Codes\\Shopping Ads\\Klarna\\Vodafone\\amp_klarna_vodafone_uk.tsv.gz\n",
      "✅ Uploaded: C:\\Users\\ywang\\Documents\\Codes\\Shopping Ads\\Klarna\\Vodafone\\amp_klarna_vodafone_uk.tsv.gz -> /files/amp_klarna_vodafone_uk.tsv.gz\n",
      "🔍 Searching for: C:\\Users\\ywang\\Documents\\Codes\\Shopping Ads\\Klarna\\BedBathBeyond\\amp_klarna_bedbathbeyond_us.tsv.gz\n",
      "✅ Uploaded: C:\\Users\\ywang\\Documents\\Codes\\Shopping Ads\\Klarna\\BedBathBeyond\\amp_klarna_bedbathbeyond_us.tsv.gz -> /files/amp_klarna_bedbathbeyond_us.tsv.gz\n",
      "🔍 Searching for: C:\\Users\\ywang\\Documents\\Codes\\Shopping Ads\\Klarna\\Bloomingdales\\amp_klarna_bloomingdales_us.tsv.gz\n",
      "✅ Uploaded: C:\\Users\\ywang\\Documents\\Codes\\Shopping Ads\\Klarna\\Bloomingdales\\amp_klarna_bloomingdales_us.tsv.gz -> /files/amp_klarna_bloomingdales_us.tsv.gz\n",
      "🔍 Searching for: C:\\Users\\ywang\\Documents\\Codes\\Shopping Ads\\Klarna\\HarryDavid\\amp_klarna_harrydavid_us.tsv.gz\n",
      "✅ Uploaded: C:\\Users\\ywang\\Documents\\Codes\\Shopping Ads\\Klarna\\HarryDavid\\amp_klarna_harrydavid_us.tsv.gz -> /files/amp_klarna_harrydavid_us.tsv.gz\n",
      "🔍 Searching for: C:\\Users\\ywang\\Documents\\Codes\\Shopping Ads\\Klarna\\Houzz\\amp_klarna_houzz_us.tsv.gz\n",
      "✅ Uploaded: C:\\Users\\ywang\\Documents\\Codes\\Shopping Ads\\Klarna\\Houzz\\amp_klarna_houzz_us.tsv.gz -> /files/amp_klarna_houzz_us.tsv.gz\n",
      "🔍 Searching for: C:\\Users\\ywang\\Documents\\Codes\\Shopping Ads\\Klarna\\NewBalance\\amp_klarna_newbalance_us.tsv.gz\n",
      "✅ Uploaded: C:\\Users\\ywang\\Documents\\Codes\\Shopping Ads\\Klarna\\NewBalance\\amp_klarna_newbalance_us.tsv.gz -> /files/amp_klarna_newbalance_us.tsv.gz\n",
      "🔍 Searching for: C:\\Users\\ywang\\Documents\\Codes\\Shopping Ads\\Klarna\\TheHomeDepot\\amp_klarna_thehomedepot_us.tsv.gz\n",
      "✅ Uploaded: C:\\Users\\ywang\\Documents\\Codes\\Shopping Ads\\Klarna\\TheHomeDepot\\amp_klarna_thehomedepot_us.tsv.gz -> /files/amp_klarna_thehomedepot_us.tsv.gz\n",
      "🔍 Searching for: C:\\Users\\ywang\\Documents\\Codes\\Shopping Ads\\Klarna\\TommyBahama\\amp_klarna_tommybahama_us.tsv.gz\n",
      "✅ Uploaded: C:\\Users\\ywang\\Documents\\Codes\\Shopping Ads\\Klarna\\TommyBahama\\amp_klarna_tommybahama_us.tsv.gz -> /files/amp_klarna_tommybahama_us.tsv.gz\n",
      "🔍 Searching for: C:\\Users\\ywang\\Documents\\Codes\\Shopping Ads\\Klarna\\Ulta\\amp_klarna_ulta_us.tsv.gz\n",
      "✅ Uploaded: C:\\Users\\ywang\\Documents\\Codes\\Shopping Ads\\Klarna\\Ulta\\amp_klarna_ulta_us.tsv.gz -> /files/amp_klarna_ulta_us.tsv.gz\n",
      "🔍 Searching for: C:\\Users\\ywang\\Documents\\Codes\\Shopping Ads\\Klarna\\Verizon\\amp_klarna_verizon_us.tsv.gz\n",
      "✅ Uploaded: C:\\Users\\ywang\\Documents\\Codes\\Shopping Ads\\Klarna\\Verizon\\amp_klarna_verizon_us.tsv.gz -> /files/amp_klarna_verizon_us.tsv.gz\n",
      "🔍 Searching for: C:\\Users\\ywang\\Documents\\Codes\\Shopping Ads\\Klarna\\Wayfair\\amp_klarna_wayfair_us.tsv.gz\n",
      "✅ Uploaded: C:\\Users\\ywang\\Documents\\Codes\\Shopping Ads\\Klarna\\Wayfair\\amp_klarna_wayfair_us.tsv.gz -> /files/amp_klarna_wayfair_us.tsv.gz\n",
      "🔍 Searching for: C:\\Users\\ywang\\Documents\\Codes\\Shopping Ads\\Klarna\\Zappos\\amp_klarna_zappos_us.tsv.gz\n",
      "✅ Uploaded: C:\\Users\\ywang\\Documents\\Codes\\Shopping Ads\\Klarna\\Zappos\\amp_klarna_zappos_us.tsv.gz -> /files/amp_klarna_zappos_us.tsv.gz\n",
      "🔍 Searching for: C:\\Users\\ywang\\Documents\\Codes\\Shopping Ads\\Klarna\\UnderArmour\\amp_klarna_underarmour_us.tsv.gz\n",
      "✅ Uploaded: C:\\Users\\ywang\\Documents\\Codes\\Shopping Ads\\Klarna\\UnderArmour\\amp_klarna_underarmour_us.tsv.gz -> /files/amp_klarna_underarmour_us.tsv.gz\n",
      "🚀 All processed files uploaded (or skipped if not found).\n",
      "CPU times: total: 2min 19s\n",
      "Wall time: 16min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import paramiko\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Define advertiser-specific settings\n",
    "advertisers = {\n",
    "    \"LookFantastic_UK\": \"/Volumes/T9/AMP/KlarnaShoppingAds/Look Fantastic/UK/\",\n",
    "    \"LookFantastic_FR\": \"/Volumes/T9/AMP/KlarnaShoppingAds/Look Fantastic/FR/\",\n",
    "    \"LookFantastic_IT\": \"/Volumes/T9/AMP/KlarnaShoppingAds/Look Fantastic/IT/\",\n",
    "    \"Sephora_UK\": \"/Volumes/T9/AMP/KlarnaShoppingAds/Sephora/\",\n",
    "    \"MyProtein_UK\": \"/Volumes/T9/AMP/KlarnaShoppingAds/My Protein/UK/\",\n",
    "    \"Vodafone_UK\": \"/Volumes/T9/AMP/KlarnaShoppingAds/Vodafone/\",\n",
    "    \"BedBathBeyond_US\": \"/Volumes/T9/AMP/KlarnaShoppingAds/BedBathBeyond/\",\n",
    "    \"Bloomingdales_US\": \"/Volumes/T9/AMP/KlarnaShoppingAds/Bloomingdales/\",\n",
    "    \"HarryDavid_US\": \"/Volumes/T9/AMP/KlarnaShoppingAds/HarryDavid/\",\n",
    "    \"Houzz_US\": \"/Volumes/T9/AMP/KlarnaShoppingAds/Houzz/\",\n",
    "    \"NewBalance_US\": \"/Volumes/T9/AMP/KlarnaShoppingAds/NewBalance/\",\n",
    "    \"TheHomeDepot_US\": \"/Volumes/T9/AMP/KlarnaShoppingAds/TheHomeDepot/\",\n",
    "    \"TommyBahama_US\": \"/Volumes/T9/AMP/KlarnaShoppingAds/TommyBahama/\",\n",
    "    \"Ulta_US\": \"/Volumes/T9/AMP/KlarnaShoppingAds/Ulta/\",\n",
    "    \"Verizon_US\": \"/Volumes/T9/AMP/KlarnaShoppingAds/Verizon/\",\n",
    "    \"Wayfair_US\": \"/Volumes/T9/AMP/KlarnaShoppingAds/Wayfair/\",\n",
    "    \"Zappos_US\": \"/Volumes/T9/AMP/KlarnaShoppingAds/Zappos/\",  # ✅ Fixed\n",
    "    \"UnderArmour_US\": \"/Volumes/T9/AMP/KlarnaShoppingAds/UnderArmour/\",\n",
    "    \"Nike_UK\": \"/Volumes/T9/AMP/KlarnaShoppingAds/Nike/UK/\"\n",
    "}\n",
    "\n",
    "# SFTP Configuration\n",
    "sftp_host = \"ftp.admarketplace.net\"\n",
    "sftp_port = 8022\n",
    "sftp_username = \"l_klarnapricerun\"\n",
    "sftp_password = \"9ir5nukn2JGEPDC5AZsiett4\"\n",
    "sftp_target_folder = \"/files\"\n",
    "\n",
    "# Upload files to SFTP\n",
    "def upload_to_sftp(local_file_path, remote_file_path):\n",
    "    try:\n",
    "        transport = paramiko.Transport((sftp_host, sftp_port))\n",
    "        transport.connect(username=sftp_username, password=sftp_password)\n",
    "        \n",
    "        with paramiko.SFTPClient.from_transport(transport) as sftp:\n",
    "            sftp.put(local_file_path, remote_file_path)\n",
    "            print(f\"✅ Uploaded: {local_file_path} -> {remote_file_path}\")\n",
    "        \n",
    "        transport.close()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to upload {local_file_path}: {e}\")\n",
    "\n",
    "# Upload all processed files\n",
    "for advertiser, folder in advertisers.items():\n",
    "    folder_path = Path(folder)\n",
    "    file_pattern = str(folder_path / f\"amp_klarna_{advertiser.lower()}.tsv.gz\")\n",
    "    \n",
    "    print(f\"🔍 Searching for: {file_pattern}\")\n",
    "    matching_files = glob.glob(file_pattern)\n",
    "\n",
    "    if not matching_files:\n",
    "        print(f\"⚠️ Skipping {advertiser}: No matching files found.\")\n",
    "        continue\n",
    "\n",
    "    for local_file in matching_files:\n",
    "        if not os.path.isfile(local_file):\n",
    "            print(f\"⚠️ File not found: {local_file}\")\n",
    "            continue\n",
    "\n",
    "        remote_file_path = f\"{sftp_target_folder}/{Path(local_file).name}\"\n",
    "        upload_to_sftp(local_file, remote_file_path)\n",
    "\n",
    "print(\"🚀 All processed files uploaded (or skipped if not found).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e54df36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample file saved as: C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/Vodafone/output_sample.tsv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define input and output file paths\n",
    "input_gz_file = \"/Volumes/T9/AMP/KlarnaShoppingAds/Vodafone/amp_klarna_vodafone.tsv.gz\"  # Replace with actual file path\n",
    "output_tsv_file = \"/Volumes/T9/AMP/KlarnaShoppingAds/Vodafone/output_sample.tsv\"\n",
    "\n",
    "# Load the first 30 rows from the compressed TSV file with error handling\n",
    "df_sample = pd.read_csv(input_gz_file, sep='\\t', low_memory=False, dtype=str, nrows=30, quoting=3, compression='gzip', on_bad_lines=\"skip\")\n",
    "\n",
    "# Save the sample as a new TSV file\n",
    "df_sample.to_csv(output_tsv_file, sep='\\t', index=False)\n",
    "\n",
    "print(f\"Sample file saved as: {output_tsv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ffaaed74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File with the first 30 rows has been saved as C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/Wayfair/amp_klarna_wayfair_first_30_rows.tsv\n"
     ]
    }
   ],
   "source": [
    "# Path to your original .tsv.gz file\n",
    "file_path = '/Volumes/T9/AMP/KlarnaShoppingAds/Wayfair/amp_klarna_wayfair.tsv.gz'\n",
    "\n",
    "# Path to save the extracted 30-row TSV file\n",
    "output_file_path = file_path.replace('.tsv.gz', '_first_30_rows.tsv')\n",
    "\n",
    "# Read the first 30 rows of the .tsv.gz file\n",
    "with gzip.open(file_path, 'rt', encoding='utf-8') as file:\n",
    "    df = pd.read_csv(file, sep='\\t', nrows=30)\n",
    "\n",
    "# Save the first 30 rows as a new TSV file\n",
    "df.to_csv(output_file_path, sep='\\t', index=False)\n",
    "\n",
    "print(f\"File with the first 30 rows has been saved as {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Each Adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6de42400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded file for Sephora saved as: C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/Sephora\\feed_20250310123041.tsv\n",
      "Processed and saved: C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/Sephora\\sephora.tsv.gz\n",
      "All downloads complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import csv\n",
    "import urllib.parse\n",
    "from datetime import datetime\n",
    "\n",
    "# Dictionary of advertisers with corrected URLs\n",
    "advertisers = {\n",
    "    \"Sephora\": {\n",
    "        \"folder\": \"/Volumes/T9/AMP/KlarnaShoppingAds/Sephora\",\n",
    "        \"url\": \"http://files-as.intelligentreach.com/feedexports/1a627511-1fcb-4ea2-885c-b849e10a8688/Feel_Unique_UK_Admarketplace.tsv\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Columns to remove\n",
    "redundant_columns = {\n",
    "    \"c:GA_product_id\", \"c:nov_score\", \"c:profit_margin_flag\", \"c:returns_margin_flag\", \"c:feed_market\",\n",
    "    \"c:allocation\", \"c:stock_level_flag\", \"c:IM_product_name\", \"c:IM_beauty_brand\", \"c:IM_product_category\",\n",
    "    \"c:IM_range\", \"c:flag_plus_size\", \"c:flag_maternity\", \"c:flag_maternity\", \"product_width\", \"product_length\",\n",
    "    \"product_height\", \"shipping_weight\", \"shipping_length\", \"unit_pricing_measure\", \"unit_pricing_base_measure\",\n",
    "    \"product_review_average\", \"c:Main_Highlights\", \"c:Shortened_Name_Ads\", \"c:Alternative Image URL (1)\",\n",
    "    \"c:display ads link\", \"display ads title\", \"adwords_labels\", \"adwords_grouping\", \"c:Was Price (inc VAT)\",\n",
    "    \"c:ultimate_price\", \"promotion_id\", \"subscription_cost\", \"instalment\", \"custom_label_0\", \"custom_label_1\",\n",
    "    \"custom_label_2\", \"custom_label_3\", \"custom_label_4\", \"custom_number_0\", \"custom_number_1\", \"custom_number_2\",\n",
    "    \"custom_number_3\", \"custom_number_4\", \"c:shipping(country:price:min_handling_time:max_han)\", \"shipping_label\",\n",
    "    \"shopping_ads_excluded_country\", \"excluded_destination\", \"return_policy_label\", \"max_handling_time\",\n",
    "    \"min_handling_time\", \"isbn\", \"identifier exists\"\n",
    "}\n",
    "\n",
    "# Generate timestamp for unique filenames\n",
    "current_timestamp = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "# Process each advertiser\n",
    "for advertiser, data in advertisers.items():\n",
    "    folder_path = data[\"folder\"]\n",
    "    url = data[\"url\"]\n",
    "\n",
    "    # Ensure folder exists\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # Determine file extension from URL\n",
    "    if url.endswith(\".csv\"):\n",
    "        extension = \".csv\"\n",
    "        delimiter = \",\"  # CSV files use commas\n",
    "    elif url.endswith(\".tsv\"):\n",
    "        extension = \".tsv\"\n",
    "        delimiter = \"\\t\"  # TSV files use tabs\n",
    "    else:\n",
    "        extension = \".txt\"  # Default to .txt if unknown\n",
    "        delimiter = \" \"  # Assume space-separated if structured\n",
    "\n",
    "    # Generate input and output file names\n",
    "    input_file_name = f'feed_{current_timestamp}{extension}'\n",
    "    output_file_name = f'{advertiser.lower()}.tsv.gz'  # Save output as .tsv.gz\n",
    "\n",
    "    # Full file paths\n",
    "    input_file_path = os.path.join(folder_path, input_file_name)\n",
    "    output_file_path = os.path.join(folder_path, output_file_name)\n",
    "\n",
    "    # Download the file\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Save the raw downloaded content\n",
    "        with open(input_file_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"Downloaded file for {advertiser} saved as: {input_file_path}\")\n",
    "\n",
    "        # Convert content to TSV format\n",
    "        df = pd.read_csv(input_file_path, sep=delimiter, low_memory=False, dtype=str, on_bad_lines=\"skip\")\n",
    "        \n",
    "        # Remove redundant columns\n",
    "        df = df.drop(columns=[col for col in redundant_columns if col in df.columns], errors='ignore')\n",
    "        \n",
    "        # Save the processed file as a compressed TSV\n",
    "        df.to_csv(output_file_path, sep='\\t', index=False, compression='gzip')\n",
    "\n",
    "        print(f\"Processed and saved: {output_file_path}\")\n",
    "\n",
    "    except requests.HTTPError as e:\n",
    "        print(f\"HTTP error occurred while downloading {advertiser}: {e}\")\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Error: Could not decode file for {advertiser}. Please check encoding.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error while processing {advertiser}: {e}\")\n",
    "\n",
    "print(\"All downloads complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaad2b5-d5a3-4af1-8dd6-f4046e1c0568",
   "metadata": {},
   "source": [
    "## Functions for Each Adv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21db1d0c-5bed-475d-8b63-1fb1c88d0fed",
   "metadata": {},
   "source": [
    "### BedBathBeyond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baaf5608-d113-43e2-a650-5bdd57bcf00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3min 42s\n",
      "Wall time: 3min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import gzip\n",
    "import csv\n",
    "\n",
    "def unzip_gz_to_csv(gz_file_path, output_csv_file_path):\n",
    "    # Open the gz file in text mode with UTF-8 encoding\n",
    "    with gzip.open(gz_file_path, 'rt', encoding='utf-8') as gz_file:\n",
    "        # Open the output CSV file in write mode\n",
    "        with open(output_csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "            reader = csv.reader(gz_file, delimiter=',')  # Assume it's comma-separated\n",
    "            writer = csv.writer(csv_file, delimiter=',')  # Writing CSV format\n",
    "\n",
    "            for row in reader:\n",
    "                # Write each row to the CSV file\n",
    "                writer.writerow(row)\n",
    "\n",
    "# Example usage\n",
    "gz_file_path = '/Volumes/T9/AMP/KlarnaShoppingAds/BedBathBeyond/BedBathBeyond_PLA.csv.gz'\n",
    "output_csv_file_path = '/Volumes/T9/AMP/KlarnaShoppingAds/BedBathBeyond/BBB_admarketplace.csv'\n",
    "\n",
    "unzip_gz_to_csv(gz_file_path, output_csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "503d3ee7-5fcc-4529-9364-fd02cce16e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV has been converted to TSV, and double quotes have been removed. Cleaned file saved at: C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/BedBathBeyond/BBB_admarketplace.tsv\n",
      "CPU times: total: 4min 26s\n",
      "Wall time: 4min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Path to your original CSV file\n",
    "csv_file_path = '/Volumes/T9/AMP/KlarnaShoppingAds/BedBathBeyond/BBB_admarketplace.csv'\n",
    "\n",
    "# Output TSV file path\n",
    "tsv_file_path = csv_file_path.replace('.csv', '.tsv')\n",
    "\n",
    "# Function to remove double quotes from all values in the dataframe\n",
    "def remove_double_quotes(chunk):\n",
    "    return chunk.apply(lambda col: col.map(lambda x: x.replace('\"', '') if isinstance(x, str) else x))\n",
    "\n",
    "# Step 1: Convert CSV to TSV\n",
    "chunksize = 10000  # Process 10,000 rows at a time\n",
    "\n",
    "with pd.read_csv(csv_file_path, sep=',', quotechar='\"', quoting=csv.QUOTE_ALL, low_memory=False, dtype=str, chunksize=chunksize) as reader:\n",
    "    for i, chunk in enumerate(reader):\n",
    "        # Step 2: Remove double quotes from the chunk\n",
    "        chunk = remove_double_quotes(chunk)\n",
    "\n",
    "        # Step 3: Write the chunk to the TSV file\n",
    "        chunk.to_csv(tsv_file_path, sep='\\t', index=False, mode='w' if i == 0 else 'a', \n",
    "                     header=(i == 0), quoting=csv.QUOTE_NONE, escapechar='\\\\')\n",
    "\n",
    "# Step 4: Confirm that the file has been saved\n",
    "print(f\"CSV has been converted to TSV, and double quotes have been removed. Cleaned file saved at: {tsv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3d4c36-2a40-4eb1-b1c8-57d4284e7cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File with updated links has been saved as C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/BedBathBeyond\\amp_klarna_bedbathbeyond_us.tsv.gz\n",
      "CPU times: total: 8min 49s\n",
      "Wall time: 9min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import os\n",
    "\n",
    "# Path to your original TSV file\n",
    "file_path = '/Volumes/T9/AMP/KlarnaShoppingAds/BedBathBeyond/BBB_admarketplace.tsv'\n",
    "\n",
    "# Base URL to append\n",
    "base_url = 'https://klarnashoppingads.ampxdirect.com/?plid=9z0zxe52a9&ctaid=25116&v=1.3&source=als_tiles'\n",
    "\n",
    "# Function to encode the link and append it to the base URL\n",
    "def create_new_link(original_link):\n",
    "    encoded_link = urllib.parse.quote_plus(original_link)\n",
    "    return f\"{base_url}&cu={encoded_link}&fbu={encoded_link}\"\n",
    "\n",
    "# Output file path with .tsv.gz\n",
    "output_file_path = os.path.join(os.path.dirname(file_path), 'amp_klarna_bedbathbeyond_us.tsv.gz')\n",
    "\n",
    "# Process the TSV file in chunks\n",
    "chunksize = 10000  # Process 10,000 rows at a time\n",
    "with pd.read_csv(file_path, sep='\\t', low_memory=False, dtype=str, on_bad_lines='skip', chunksize=chunksize) as reader:\n",
    "    for i, chunk in enumerate(reader):\n",
    "        # Ensure the 'Link' column is treated as strings and fill NaN with an empty string\n",
    "        if 'Link' in chunk.columns:\n",
    "            chunk['Link'] = chunk['Link'].astype(str).fillna('')\n",
    "\n",
    "            # Apply the function to create a new link\n",
    "            chunk['Link'] = chunk['Link'].apply(create_new_link)\n",
    "\n",
    "        # Append the processed chunk to the output file with gzip compression\n",
    "        if i == 0:\n",
    "            # Write the header for the first chunk\n",
    "            chunk.to_csv(output_file_path, sep='\\t', index=False, mode='w', compression='gzip')\n",
    "        else:\n",
    "            # Append subsequent chunks without writing the header\n",
    "            chunk.to_csv(output_file_path, sep='\\t', index=False, mode='a', header=False, compression='gzip')\n",
    "\n",
    "print(f\"File with updated links has been saved as {output_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddd30bc-05ad-4eba-8004-b64ce69c11dd",
   "metadata": {},
   "source": [
    "### Zappos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6375a808-a6d2-4ea3-b430-68641e04eebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File has been saved as C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/Zappos/Zappos_PLA.tsv\n",
      "CPU times: total: 1min 27s\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "# Path to your .csv.gz file\n",
    "file_path = '/Volumes/T9/AMP/KlarnaShoppingAds/Zappos/Zappos_PLA.txt.gz'\n",
    "\n",
    "# Path to save the extracted TSV file (with a .tsv extension)\n",
    "output_file_path = file_path.replace('.txt.gz', '.tsv')\n",
    "\n",
    "# Read the .gz file with error handling and specify encoding\n",
    "with gzip.open(file_path, 'rt', encoding='utf-8', errors='replace') as file:\n",
    "    df = pd.read_csv(file, sep='\\t', on_bad_lines='skip')\n",
    "\n",
    "# Save the dataframe as a TSV file (without index)\n",
    "df.to_csv(output_file_path, sep='\\t', index=False)\n",
    "\n",
    "print(f\"File has been saved as {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a48bbfd0-10ae-43cf-8064-558ea5038d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File with updated links, renamed columns, and added missing columns has been saved as C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/Zappos\\amp_klarna_zappos_us.tsv.gz\n",
      "CPU times: total: 2min 30s\n",
      "Wall time: 2min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "\n",
    "# Path to your original TSV file\n",
    "file_path = '/Volumes/T9/AMP/KlarnaShoppingAds/Zappos/zapoos_adsmarketplace.tsv'\n",
    "\n",
    "# Read the TSV file\n",
    "df = pd.read_csv(file_path, sep='\\t', low_memory=False, dtype=str)\n",
    "\n",
    "# Ensure the 'link' column is treated as strings and fill NaN with an empty string\n",
    "df['link'] = df['link'].astype(str).fillna('')\n",
    "\n",
    "# Remove anything after '?' in the 'link' column\n",
    "df['link'] = df['link'].apply(lambda x: x.split('?')[0])\n",
    "\n",
    "# Base URL to append\n",
    "base_url = 'https://klarnashoppingads.ampxdirect.com/?plid=9z0zxe52a9&ctaid=1377&v=1.3&source=als_tiles'\n",
    "\n",
    "# Function to encode the link and append it to the base URL\n",
    "def create_new_link(original_link):\n",
    "    encoded_link = urllib.parse.quote_plus(original_link)\n",
    "    new_link = f\"{base_url}&cu={encoded_link}&fbu={encoded_link}\"\n",
    "    return new_link\n",
    "\n",
    "# Apply the function to the 'link' column\n",
    "df['link'] = df['link'].apply(create_new_link)\n",
    "\n",
    "# Column renaming based on the required mapping\n",
    "column_mapping = {\n",
    "    'id': 'SKU/id',\n",
    "    'title': 'Name',\n",
    "    'description': 'Description',\n",
    "    'google_product_category': 'Category',\n",
    "    'link': 'URL',\n",
    "    'image_link': 'Image URL',\n",
    "    'condition': 'Condition',\n",
    "    'availability': 'Stock status',\n",
    "    'price': 'Price',\n",
    "    'brand': 'Manufacturer',\n",
    "    'gtin': 'EAN/GTIN',  # Ensuring GTIN remains a string\n",
    "    'mpn': 'Manufacturer SKU / MPN',\n",
    "    'gender': 'Gender',\n",
    "    'age_group': 'AgeGroup',\n",
    "    'color': 'Color',\n",
    "    'size': 'Size',\n",
    "    'item_group_id': 'GroupId',\n",
    "    'material': 'Material',\n",
    "    'pattern': 'Pattern',\n",
    "    'shipping': 'Shipping costs'  # Adjust if this column represents shipping costs\n",
    "}\n",
    "\n",
    "# Rename columns\n",
    "df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# Ensure that 'EAN/GTIN' is treated as a string and remove any '.0' from GTIN values\n",
    "df['SKU/id'] = df['SKU/id'].astype(str).apply(lambda x: x.rstrip('.0') if '.0' in x else x)\n",
    "df['EAN/GTIN'] = df['EAN/GTIN'].astype(str).apply(lambda x: x.rstrip('.0') if '.0' in x else x)\n",
    "\n",
    "# Step to handle numeric columns that show decimal\n",
    "# Identify columns that can be safely converted to integers, excluding 'EAN/GTIN'\n",
    "numeric_cols = df.columns[df.apply(lambda col: col.str.isnumeric(), axis=0).all()]\n",
    "numeric_cols = numeric_cols.drop('EAN/GTIN', errors='ignore')  # Exclude 'EAN/GTIN'\n",
    "\n",
    "# Convert those columns to integers explicitly\n",
    "df[numeric_cols] = df[numeric_cols].apply(lambda x: pd.to_numeric(x, errors='coerce').fillna(0).astype(int))\n",
    "\n",
    "# List of missing columns based on the requirements\n",
    "missing_columns = ['AdultContent', 'Delivery time', 'Bundled', 'EnergyEfficiencyClass', 'Multipack', 'SizeSystem']\n",
    "\n",
    "# Add missing columns with empty values or default values\n",
    "for col in missing_columns:\n",
    "    df[col] = ''  # Set as empty or default as needed\n",
    "\n",
    "# Save the updated dataframe with renamed columns and new field)\n",
    "\n",
    "output_file_path = os.path.join(os.path.dirname(file_path), 'amp_klarna_zappos_us.tsv.gz')\n",
    "df.to_csv(output_file_path, sep='\\t', index=False, compression='gzip')\n",
    "\n",
    "print(f\"File with updated links, renamed columns, and added missing columns has been saved as {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a23717",
   "metadata": {},
   "source": [
    "Under Armour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a798fb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:78: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "<timed exec>:78: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "<timed exec>:78: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File with updated links, renamed columns, and added missing columns has been saved as C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/UnderArmour\\amp_klarna_underarmour_us.tsv.gz\n",
      "CPU times: total: 5.61 s\n",
      "Wall time: 5.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import paramiko\n",
    "import os\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "from datetime import datetime\n",
    "\n",
    "# Path to your original CSV file\n",
    "local_file_path = '/Volumes/T9/AMP/KlarnaShoppingAds/UnderArmour/UnderArmour_PLA.csv'\n",
    "\n",
    "# Proceed with the rest of the code to process the file\n",
    "df = pd.read_csv(local_file_path, sep=',', low_memory=False, dtype=str)\n",
    "\n",
    "# Ensure the 'Link' column is treated as strings and fill NaN with an empty string\n",
    "df['Link'] = df['Link'].astype(str).fillna('')\n",
    "\n",
    "# Base URL to append\n",
    "base_url = 'https://klarnashoppingads.ampxdirect.com/?plid=9z0zxe52a9&ctaid=1145&v=1.3&source=als_tiles'\n",
    "\n",
    "# Function to encode the link and append it to the base URL\n",
    "def create_new_link(original_link):\n",
    "    encoded_link = urllib.parse.quote_plus(original_link)\n",
    "    new_link = f\"{base_url}&cu={encoded_link}&fbu={encoded_link}\"\n",
    "    return new_link\n",
    "\n",
    "# Apply the function to the 'Link' column\n",
    "df['Link'] = df['Link'].apply(create_new_link)\n",
    "\n",
    "# Column renaming based on the required mapping\n",
    "column_mapping = {\n",
    "    'ID': 'SKU/id',\n",
    "    'Title': 'Name',\n",
    "    'Description': 'Description',\n",
    "    'Link': 'URL',\n",
    "    'Image Link': 'Image URL',\n",
    "    'Condition': 'Condition',\n",
    "    'Availability': 'Stock status',\n",
    "    'Price': 'Price',\n",
    "    'Brand': 'Manufacturer',\n",
    "    'GTIN': 'EAN/GTIN',\n",
    "    'MPN': 'Manufacturer SKU / MPN',\n",
    "    'Gender': 'Gender',\n",
    "    'Age Group': 'AgeGroup',\n",
    "    'Color': 'Color',\n",
    "    'Size': 'Size',\n",
    "    'Google Product Category': 'Category',\n",
    "    'Sale Price': 'Sale Price',\n",
    "    'Sale Price Effective Date': 'Sale Price Effective Date',\n",
    "    'Expiration Date': 'Expiration Date',\n",
    "    'Mobile Link': 'Mobile Link'\n",
    "}\n",
    "\n",
    "# Rename columns based on the mapping\n",
    "df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# List of columns to convert to integers to remove '.0'\n",
    "columns_to_convert = ['SKU/id', 'EAN/GTIN']\n",
    "\n",
    "# Function to remove '.0' by converting to integer where possible\n",
    "def remove_decimal(value):\n",
    "    try:\n",
    "        value_float = float(value)\n",
    "        if value_float.is_integer():\n",
    "            return str(int(value_float))\n",
    "        return value\n",
    "    except (ValueError, TypeError):\n",
    "        return value\n",
    "\n",
    "# Apply the function to the specified columns\n",
    "for col in columns_to_convert:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(remove_decimal)\n",
    "\n",
    "# Identify other numeric columns that can be safely converted to integers\n",
    "numeric_cols = df.select_dtypes(include=['object']).columns.difference(columns_to_convert)\n",
    "numeric_cols = numeric_cols[df[numeric_cols].apply(lambda col: col.str.isnumeric().all())]\n",
    "\n",
    "# Convert those columns to integers explicitly\n",
    "df[numeric_cols] = df[numeric_cols].apply(lambda x: x.fillna(0).astype(int))\n",
    "\n",
    "# List of missing columns based on the requirements\n",
    "missing_columns = ['AdultContent', 'Delivery time', 'Bundled', 'EnergyEfficiencyClass', 'Multipack', 'SizeSystem']\n",
    "\n",
    "# Add missing columns with empty values or default values\n",
    "for col in missing_columns:\n",
    "    df[col] = ''  # Set as empty or default as needed\n",
    "\n",
    "# Save the final output as a compressed TSV.GZ file\n",
    "output_file_path = os.path.join(os.path.dirname(local_file_path), 'amp_klarna_underarmour_us.tsv.gz')\n",
    "df.to_csv(output_file_path, sep='\\t', index=False, compression='gzip')\n",
    "\n",
    "print(f\"File with updated links, renamed columns, and added missing columns has been saved as {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a01107-2a2c-4a5c-8f34-d56996786945",
   "metadata": {},
   "source": [
    "### Bloomingdales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b1ff319-5c0f-4a09-83c6-7bad0f834c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:79: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "<timed exec>:79: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File with updated links, renamed columns, and added missing columns has been saved as C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/Bloomingdales\\amp_klarna_bloomingdales_us.tsv.gz\n",
      "CPU times: total: 51.9 s\n",
      "Wall time: 52.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import paramiko\n",
    "import os\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Path to your original TSV file\n",
    "local_file_path = '/Volumes/T9/AMP/KlarnaShoppingAds/Bloomingdales/Bloomingdales_PLA.csv'\n",
    "\n",
    "# Proceed with the rest of the code to process the file\n",
    "df = pd.read_csv(local_file_path, sep=',', low_memory=False, dtype=str)\n",
    "\n",
    "# Ensure the 'Link' column is treated as strings and fill NaN with an empty string\n",
    "df['Link'] = df['Link'].astype(str).fillna('')\n",
    "\n",
    "# Base URL to append\n",
    "base_url = 'https://klarnashoppingads.ampxdirect.com/?plid=9z0zxe52a9&ctaid=1017&v=1.3&source=als_tiles'\n",
    "\n",
    "# Function to encode the link and append it to the base URL\n",
    "def create_new_link(original_link):\n",
    "    encoded_link = urllib.parse.quote_plus(original_link)\n",
    "    new_link = f\"{base_url}&cu={encoded_link}&fbu={encoded_link}\"\n",
    "    return new_link\n",
    "\n",
    "# Apply the function to the 'Link' column\n",
    "df['Link'] = df['Link'].apply(create_new_link)\n",
    "\n",
    "# Column renaming based on the required mapping\n",
    "column_mapping = {\n",
    "    'ID': 'SKU/id',\n",
    "    'Title': 'Name',\n",
    "    'Description': 'Description',\n",
    "    'Link': 'URL',\n",
    "    'Image Link': 'Image URL',\n",
    "    'Condition': 'Condition',\n",
    "    'Availability': 'Stock status',\n",
    "    'Price': 'Price',\n",
    "    'Brand': 'Manufacturer',\n",
    "    'GTIN': 'EAN/GTIN',\n",
    "    'MPN': 'Manufacturer SKU / MPN',\n",
    "    'Gender': 'Gender',\n",
    "    'Age Group': 'AgeGroup',\n",
    "    'Color': 'Color',\n",
    "    'Size': 'Size',\n",
    "    'Google Product Category': 'Category',\n",
    "    'Sale Price': 'Sale Price',\n",
    "    'Sale Price Effective Date': 'Sale Price Effective Date',\n",
    "    'Expiration Date': 'Expiration Date',\n",
    "    'Mobile Link': 'Mobile Link'\n",
    "}\n",
    "\n",
    "# Rename columns based on the mapping\n",
    "df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# List of columns to convert to integers to remove '.0'\n",
    "columns_to_convert = ['SKU/id', 'EAN/GTIN']\n",
    "\n",
    "# Function to remove '.0' by converting to integer where possible\n",
    "def remove_decimal(value):\n",
    "    try:\n",
    "        value_float = float(value)\n",
    "        if value_float.is_integer():\n",
    "            return str(int(value_float))\n",
    "        return value\n",
    "    except (ValueError, TypeError):\n",
    "        return value\n",
    "\n",
    "# Apply the function to the specified columns\n",
    "for col in columns_to_convert:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(remove_decimal)\n",
    "\n",
    "# Identify other numeric columns that can be safely converted to integers\n",
    "numeric_cols = df.select_dtypes(include=['object']).columns.difference(columns_to_convert)\n",
    "numeric_cols = numeric_cols[df[numeric_cols].apply(lambda col: col.str.isnumeric().all())]\n",
    "\n",
    "# Convert those columns to integers explicitly\n",
    "df[numeric_cols] = df[numeric_cols].apply(lambda x: x.fillna(0).astype(int))\n",
    "\n",
    "# List of missing columns based on the requirements\n",
    "missing_columns = ['AdultContent', 'Delivery time', 'Bundled', 'EnergyEfficiencyClass', 'Multipack', 'SizeSystem']\n",
    "\n",
    "# Add missing columns with empty values or default values\n",
    "for col in missing_columns:\n",
    "    df[col] = ''  # Set as empty or default as needed\n",
    "\n",
    "# Save the final output as a compressed TSV.GZ file\n",
    "output_file_path = os.path.join(os.path.dirname(local_file_path), 'amp_klarna_bloomingdales_us.tsv.gz')\n",
    "df.to_csv(output_file_path, sep='\\t', index=False, compression='gzip')\n",
    "\n",
    "print(f\"File with updated links, renamed columns, and added missing columns has been saved as {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065ad4a6-d997-441b-9415-476e0053e072",
   "metadata": {},
   "source": [
    "### Verizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2282bc3-390e-4645-9463-9ddff07f4e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded from SFTP and saved locally as C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/Verizon\\verizon_devices_admarketplace.csv\n",
      "Processed file has been saved as C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/Verizon\\amp_klarna_verizon_us.tsv.gz\n",
      "CPU times: total: 688 ms\n",
      "Wall time: 1.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import paramiko\n",
    "import os\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "from datetime import datetime\n",
    "\n",
    "# SFTP credentials and connection details\n",
    "sftp_host = 'ftp.admarketplace.net'\n",
    "sftp_port = 8022  # Default port for SFTP\n",
    "username = 'ywang'\n",
    "password = '123456789'  # Recommend using environment variables for credentials\n",
    "\n",
    "# Establish SFTP connection\n",
    "try:\n",
    "    transport = paramiko.Transport((sftp_host, sftp_port))\n",
    "    transport.connect(username=username, password=password)\n",
    "    sftp = paramiko.SFTPClient.from_transport(transport)\n",
    "\n",
    "    # Navigate to the directory where the file is located\n",
    "    target_directory = '/sftp/l_verizon/files/'  # Ensure this is the correct directory\n",
    "    sftp.chdir(target_directory)\n",
    "\n",
    "    # File naming pattern (assumed static here, but adjust if it varies)\n",
    "    input_file_name = 'verizon_devices_admarketplace.csv'\n",
    "    local_file_path = os.path.join('/Volumes/T9/AMP/KlarnaShoppingAds/Verizon', input_file_name)\n",
    "\n",
    "    # Download the file from SFTP to your local system\n",
    "    sftp.get(input_file_name, local_file_path)\n",
    "    print(f\"File downloaded from SFTP and saved locally as {local_file_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    if sftp:\n",
    "        sftp.close()\n",
    "    if transport:\n",
    "        transport.close()\n",
    "\n",
    "# Check if the file exists and is not empty\n",
    "if os.path.exists(local_file_path) and os.path.getsize(local_file_path) > 0:\n",
    "    # The file exists and has data, so let's process it\n",
    "    try:\n",
    "        df = pd.read_csv(local_file_path, low_memory=False, dtype=str)\n",
    "        df['link'] = df['link'].astype(str).fillna('')\n",
    "\n",
    "        # Base URL to append\n",
    "        base_url = 'https://klarnashoppingads.ampxdirect.com/?plid=9z0zxe52a9&ctaid=1149&v=1.3&source=als_tiles'\n",
    "\n",
    "        # Function to encode the link and append it to the base URL\n",
    "        def create_new_link(original_link):\n",
    "            encoded_link = urllib.parse.quote_plus(original_link)\n",
    "            new_link = f\"{base_url}&cu={encoded_link}&fbu={encoded_link}\"\n",
    "            return new_link\n",
    "\n",
    "        # Apply the function to the 'link' column\n",
    "        df['link'] = df['link'].apply(create_new_link)\n",
    "\n",
    "        # Column renaming based on the required mapping\n",
    "        column_mapping = {\n",
    "            'id': 'SKU/id',\n",
    "            'title': 'Name',\n",
    "            'description': 'Description',\n",
    "            'google_product_category': 'Category',\n",
    "            'product_type': 'Product Type',\n",
    "            'link': 'URL',\n",
    "            'image_link': 'Image URL',\n",
    "            'condition': 'Condition',\n",
    "            'availability': 'Stock status',\n",
    "            'price': 'Price',\n",
    "            'brand': 'Manufacturer',\n",
    "            'gtin': 'EAN/GTIN',\n",
    "            'mpn': 'Manufacturer SKU / MPN',\n",
    "            'color': 'Color',\n",
    "            'size': 'Size',\n",
    "            'shipping': 'Shipping costs',\n",
    "            'custom_label_0': 'Custom Label 0',\n",
    "            'custom_label_1': 'Custom Label 1',\n",
    "            'custom_label_2': 'Custom Label 2',\n",
    "            'custom_label_3': 'Custom Label 3',\n",
    "            'custom_label_4': 'Custom Label 4',\n",
    "            'short_title': 'Short Title',\n",
    "            'gender': 'Gender',\n",
    "            'age_group': 'AgeGroup',\n",
    "            'installment': 'Installment',\n",
    "            'availability_date': 'Availability Date'\n",
    "        }\n",
    "\n",
    "        # Rename columns\n",
    "        df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "        # Function to remove decimals from the SKU and GTIN columns\n",
    "        def remove_decimal(value):\n",
    "            try:\n",
    "                value_float = float(value)\n",
    "                if value_float.is_integer():\n",
    "                    return str(int(value_float))\n",
    "                return value\n",
    "            except (ValueError, TypeError):\n",
    "                return value\n",
    "\n",
    "        # Apply this function to necessary columns\n",
    "        columns_to_convert = ['SKU/id', 'EAN/GTIN']\n",
    "        for col in columns_to_convert:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].apply(remove_decimal)\n",
    "\n",
    "        # Output file path and saving as TSV\n",
    "        output_file_path = os.path.join(os.path.dirname(local_file_path), 'amp_klarna_verizon_us.tsv.gz')\n",
    "        df.to_csv(output_file_path, sep='\\t', index=False, compression='gzip')\n",
    "\n",
    "        print(f\"Processed file has been saved as {output_file_path}\")\n",
    "\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"Error: The CSV file is empty.\")\n",
    "else:\n",
    "    print(f\"Error: The file {local_file_path} does not exist or is empty.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81737f5-9e32-44e1-bde0-aaa835d5ad94",
   "metadata": {},
   "source": [
    "### New Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c4d2ef-097b-46ab-9b47-fc985ab3f511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_newbalance(df):\n",
    "    try:\n",
    "        df = pd.read_csv(local_file_path, low_memory=False, dtype=str)\n",
    "        df['link'] = df['link'].astype(str).fillna('')\n",
    "\n",
    "        # Base URL to append\n",
    "        base_url = 'https://klarnashoppingads.ampxdirect.com/?plid=9z0zxe52a9&ctaid=1335&v=1.3&source=als_tiles'\n",
    "\n",
    "        # Function to encode the link and append it to the base URL\n",
    "        def create_new_link(original_link):\n",
    "            encoded_link = urllib.parse.quote_plus(original_link)\n",
    "            new_link = f\"{base_url}&cu={encoded_link}&fbu={encoded_link}\"\n",
    "            return new_link\n",
    "\n",
    "        # Apply the function to the 'link' column\n",
    "        df['link'] = df['link'].apply(create_new_link)\n",
    "\n",
    "        # Column renaming based on the required mapping\n",
    "        column_mapping = {\n",
    "            'GTIN': 'EAN/GTIN',\n",
    "            'MPN': 'Manufacturer SKU / MPN',\n",
    "            'ID': 'SKU/id',\n",
    "            'Link': 'URL',\n",
    "            'Title': 'Name',\n",
    "            'Description': 'Description',\n",
    "            'Image Link': 'Image URL',\n",
    "            'Price': 'Price',\n",
    "            'Condition': 'Condition',\n",
    "            'Availability': 'Stock status',\n",
    "            'Brand': 'Manufacturer',\n",
    "            'Google Product Category': 'Category',\n",
    "            'Top Performing Product': 'Bundled',  # Assuming relation\n",
    "            'Color': 'Color',\n",
    "            'Size': 'Size',\n",
    "            'Gender': 'Gender',\n",
    "            'Age Group': 'AgeGroup',\n",
    "            'Sale Price': 'Sale Price',\n",
    "            'Sale Price Effective Date': 'Sale Price Effective Date',\n",
    "            'Expiration Date': 'Expiration Date',\n",
    "    # Additional mappings from second part\n",
    "            'SizeSystem': 'SizeSystem',\n",
    "            'AdultContent': 'AdultContent',\n",
    "            'Delivery time': 'Delivery time',  # Mapped directly from the second part\n",
    "            'EnergyEfficiencyClass': 'EnergyEfficiencyClass',\n",
    "            'GroupId': 'GroupId',\n",
    "            'Material': 'Material',\n",
    "            'Multipack': 'Multipack',\n",
    "            'Pattern': 'Pattern'\n",
    "        }\n",
    "\n",
    "\n",
    "        # Rename columns\n",
    "        df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "        # Function to remove decimals from the SKU and GTIN columns\n",
    "        def remove_decimal(value):\n",
    "            try:\n",
    "                value_float = float(value)\n",
    "                if value_float.is_integer():\n",
    "                    return str(int(value_float))\n",
    "                return value\n",
    "            except (ValueError, TypeError):\n",
    "                return value\n",
    "\n",
    "        # Apply this function to necessary columns\n",
    "        columns_to_convert = ['SKU/id', 'EAN/GTIN']\n",
    "        for col in columns_to_convert:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].apply(remove_decimal)\n",
    "\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"Error: The CSV file is empty.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {local_file_path} does not exist or is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fa79f6",
   "metadata": {},
   "source": [
    "Ulta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c285b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File with updated links, renamed columns, and added missing columns has been saved as C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/Ulta\\amp_klarna_ulta.tsv.gz\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import os\n",
    "\n",
    "# Path to your original TSV file\n",
    "file_path = '/Volumes/T9/AMP/KlarnaShoppingAds/Ulta/ulta.tsv'\n",
    "\n",
    "# Read the TSV file\n",
    "df = pd.read_csv(file_path, sep='\\t', low_memory=False, dtype=str)\n",
    "\n",
    "# Ensure the 'link' column is treated as strings and fill NaN with an empty string\n",
    "df['link'] = df['link'].astype(str).fillna('')\n",
    "\n",
    "# Base URL to append\n",
    "base_url = 'https://klarnashoppingads.ampxdirect.com/?partner=klarnashoppingads&sub1=shoppingads&ctaid=74843&v=1.3&source=als_tiles'\n",
    "\n",
    "# Function to encode the link and append it to the base URL\n",
    "def create_new_link(original_link):\n",
    "    encoded_link = urllib.parse.quote_plus(original_link)\n",
    "    new_link = f\"{base_url}&cu={encoded_link}&fbu={encoded_link}\"\n",
    "    return new_link\n",
    "\n",
    "# Apply the function to the 'link' column\n",
    "df['link'] = df['link'].apply(create_new_link)\n",
    "\n",
    "# Column renaming based on the required mapping\n",
    "column_mapping = {\n",
    "    'id': 'SKU/id',\n",
    "    'title': 'Name',\n",
    "    'price': 'Price',\n",
    "    'shipping': 'Shipping costs',  \n",
    "    'availability': 'Stock status',  \n",
    "    'availability_date': 'Delivery time',  \n",
    "    'brand': 'Manufacturer',\n",
    "    'gtin': 'EAN/GTIN',\n",
    "    'mpn': 'Manufacturer SKU / MPN',\n",
    "    'link': 'URL',\n",
    "    'image_link': 'Image URL',\n",
    "    'google_product_category': 'Category',\n",
    "    'description': 'Description',\n",
    "    'adult': 'AdultContent',\n",
    "    'age_group': 'AgeGroup',\n",
    "    'color': 'Color',\n",
    "    'condition': 'Condition',\n",
    "    'item_group_id': 'GroupId',\n",
    "    'material': 'Material',\n",
    "    'pattern': 'Pattern',\n",
    "    'size': 'Size',\n",
    "    'size_system': 'SizeSystem',\n",
    "}\n",
    "\n",
    "# Rename columns\n",
    "df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# Ensure that 'EAN/GTIN' is treated as a string and remove any '.0' from GTIN values\n",
    "df['EAN/GTIN'] = df['EAN/GTIN'].astype(str).apply(lambda x: x.rstrip('.0') if '.0' in x else x)\n",
    "\n",
    "# Convert numeric columns to integers\n",
    "numeric_cols = df.columns[df.apply(lambda col: col.str.isnumeric(), axis=0).all()]\n",
    "numeric_cols = numeric_cols.drop('EAN/GTIN', errors='ignore')  \n",
    "\n",
    "# Convert numeric columns to integers explicitly\n",
    "df[numeric_cols] = df[numeric_cols].apply(lambda x: pd.to_numeric(x, errors='coerce').fillna(0).astype(int))\n",
    "\n",
    "# List of missing columns\n",
    "missing_columns = ['AdultContent', 'Delivery time', 'Bundled', 'EnergyEfficiencyClass', 'Multipack', 'SizeSystem']\n",
    "\n",
    "# Add missing columns with default values\n",
    "for col in missing_columns:\n",
    "    df[col] = ''  # Set as empty or default as needed\n",
    "\n",
    "# Set the output file path with `.tsv.gz`\n",
    "output_file_path = os.path.join(os.path.dirname(file_path), 'amp_klarna_ulta_us.tsv.gz')\n",
    "\n",
    "# Save the DataFrame as a compressed TSV (.tsv.gz)\n",
    "df.to_csv(output_file_path, sep='\\t', index=False, compression='gzip')\n",
    "\n",
    "print(f\"File with updated links, renamed columns, and added missing columns has been saved as {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d28fdb-2f28-4e2e-952d-9f4092829c1d",
   "metadata": {},
   "source": [
    "## Proceed with File and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3be21aa-21fb-4b1b-ac43-28a47aa1554b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bloomingdales: Processed file has been saved as C:/Users/ywang/Documents/Codes/Shopping Ads/Klarna/Bloomingdales/amp_klarna_Bloomingdales.tsv.gz\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:39\u001b[0m\n",
      "File \u001b[1;32m<timed exec>:23\u001b[0m, in \u001b[0;36mprocess_file\u001b[1;34m(advertiser, local_file_path, output_file_path)\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import os\n",
    "\n",
    "# Define the processing function for each file\n",
    "def process_file(advertiser, local_file_path, output_file_path):\n",
    "    # Read the CSV file (adjust reading logic per advertiser's format if needed)\n",
    "    df = pd.read_csv(local_file_path, low_memory=False, dtype=str)\n",
    "\n",
    "    # Processing logic specific to each advertiser\n",
    "    if advertiser == 'Bloomingdales':\n",
    "        df = process_bloomingdales(df)\n",
    "        \n",
    "    elif advertiser == 'Verizon':\n",
    "        df = process_verizon(df)\n",
    "    elif advertiser == 'NewBalance':\n",
    "        df = process_newbalance(df)\n",
    "    #     \n",
    "\n",
    "    # # Add other advertiser-specific cases here (e.g., Tommy Bahama, HomeDepot, etc.)\n",
    "    \n",
    "    # Save the processed file as TSV (with gzip compression)\n",
    "    df.to_csv(output_file_path, sep='\\t', index=False, compression='gzip')\n",
    "    print(f\"{advertiser}: Processed file has been saved as {output_file_path}\")\n",
    "\n",
    "\n",
    "# Step 2: Process each file individually after downloading\n",
    "for advertiser, paths in advertisers.items():\n",
    "    local_folder = paths['local_path']\n",
    "    \n",
    "    # Generate input file name (static or dynamic)\n",
    "    input_file_name = paths['file_pattern'](current_date) if callable(paths['file_pattern']) else paths['file_pattern']\n",
    "    local_file_path = os.path.join(local_folder, input_file_name)\n",
    "    \n",
    "    # Output file for processed data\n",
    "    output_file_path = os.path.join(local_folder, f'amp_klarna_{advertiser}.tsv.gz')\n",
    "    \n",
    "    # Process the file with advertiser-specific logic\n",
    "    process_file(advertiser, local_file_path, output_file_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f8a4b2-9568-4861-bafd-53bc3499ce99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
