{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "217a16fd",
   "metadata": {},
   "source": [
    "## Download PLA File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e678106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ftplib\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "from datetime import date,timedelta\n",
    "from ftplib import FTP\n",
    "from io import StringIO\n",
    "import paramiko\n",
    "import os\n",
    "import pkg.invalid_char_clean as invalid_clean\n",
    "import pkg.kelkoo_link_transfer as kelkoo_transfer\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9ec09ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_lst = ['samsung-uk','vodafone','homebase']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cea7d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gtin_processor(gtin_num):\n",
    "    if gtin_num != gtin_num:\n",
    "        print('Null Value')\n",
    "    elif len(str(gtin_num))<13:\n",
    "        gtin_num = '0'*(13-len(str(gtin_num)))+str(gtin_num)\n",
    "    elif len(str(gtin_num))>13:\n",
    "        print(\"This number is over 13 digits: \" + str(gtin_num))\n",
    "    return str(gtin_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "089c4d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the xml_to_tsv function to exclude tags that start with 'c:'\n",
    "\n",
    "def updated_xml_to_tsv(xml_content, tsv_filename):\n",
    "    # Parse the XML content\n",
    "    root = etree.fromstring(xml_content.encode('utf-8'))\n",
    "\n",
    "    # Define the namespaces for 'g:'\n",
    "    namespaces = {\n",
    "        'g': 'http://base.google.com/ns/1.0'\n",
    "    }\n",
    "\n",
    "    # Open the TSV file for writing\n",
    "    with open(tsv_filename, 'w', encoding='utf_8_sig') as tsv_file:\n",
    "        # Define the headers from the XML structure without 'c:'\n",
    "        headers = [\n",
    "            'id', 'title', 'description', 'link', 'image_link', 'availability',\n",
    "            'price', 'google_product_category', 'product_type', 'brand', 'gtin',\n",
    "            'identifier_exists', 'condition', 'age_group', 'color', 'gender',\n",
    "            'material', 'pattern', 'size', 'custom_label_0', 'custom_label_1',\n",
    "            'custom_label_2', 'shipping_country', 'shipping_service', 'shipping_price'\n",
    "        ]\n",
    "        tsv_file.write('\\t'.join(headers) + '\\n')\n",
    "\n",
    "        # Iterate through each <item> in the XML\n",
    "        for item in root.xpath('//item'):\n",
    "            data = {\n",
    "                'id': item.xpath('.//g:id/text()', namespaces=namespaces)[0] if item.xpath('.//g:id/text()', namespaces=namespaces) else \"\",\n",
    "                'title': item.xpath('.//title/text()')[0] if item.xpath('.//title/text()') else \"\",\n",
    "                'description': item.xpath('.//description/text()')[0] if item.xpath('.//description/text()') else \"\",\n",
    "                'link': item.xpath('.//link/text()')[0] if item.xpath('.//link/text()') else \"\",\n",
    "                'image_link': item.xpath('.//g:image_link/text()', namespaces=namespaces)[0] if item.xpath('.//g:image_link/text()', namespaces=namespaces) else \"\",\n",
    "                'availability': item.xpath('.//g:availability/text()', namespaces=namespaces)[0] if item.xpath('.//g:availability/text()', namespaces=namespaces) else \"\",\n",
    "                'price': item.xpath('.//g:price/text()', namespaces=namespaces)[0] if item.xpath('.//g:price/text()', namespaces=namespaces) else \"\",\n",
    "                'google_product_category': item.xpath('.//g:google_product_category/text()', namespaces=namespaces)[0] if item.xpath('.//g:google_product_category/text()', namespaces=namespaces) else \"\",\n",
    "                'product_type': item.xpath('.//g:product_type/text()', namespaces=namespaces)[0] if item.xpath('.//g:product_type/text()', namespaces=namespaces) else \"\",\n",
    "                'brand': item.xpath('.//g:brand/text()', namespaces=namespaces)[0] if item.xpath('.//g:brand/text()', namespaces=namespaces) else \"\",\n",
    "                'gtin': item.xpath('.//g:gtin/text()', namespaces=namespaces)[0] if item.xpath('.//g:gtin/text()', namespaces=namespaces) else \"\",\n",
    "                'identifier_exists': item.xpath('.//g:identifier_exists/text()', namespaces=namespaces)[0] if item.xpath('.//g:identifier_exists/text()', namespaces=namespaces) else \"\",\n",
    "                'condition': item.xpath('.//g:condition/text()', namespaces=namespaces)[0] if item.xpath('.//g:condition/text()', namespaces=namespaces) else \"\",\n",
    "                'age_group': item.xpath('.//g:age_group/text()', namespaces=namespaces)[0] if item.xpath('.//g:age_group/text()', namespaces=namespaces) else \"\",\n",
    "                'color': item.xpath('.//g:color/text()', namespaces=namespaces)[0] if item.xpath('.//g:color/text()', namespaces=namespaces) else \"\",\n",
    "                'gender': item.xpath('.//g:gender/text()', namespaces=namespaces)[0] if item.xpath('.//g:gender/text()', namespaces=namespaces) else \"\",\n",
    "                'material': item.xpath('.//g:material/text()', namespaces=namespaces)[0] if item.xpath('.//g:material/text()', namespaces=namespaces) else \"\",\n",
    "                'pattern': item.xpath('.//g:pattern/text()', namespaces=namespaces)[0] if item.xpath('.//g:pattern/text()', namespaces=namespaces) else \"\",\n",
    "                'size': item.xpath('.//g:size/text()', namespaces=namespaces)[0] if item.xpath('.//g:size/text()', namespaces=namespaces) else \"\",\n",
    "                'custom_label_0': item.xpath('.//g:custom_label_0/text()', namespaces=namespaces)[0] if item.xpath('.//g:custom_label_0/text()', namespaces=namespaces) else \"\",\n",
    "                'custom_label_1': item.xpath('.//g:custom_label_1/text()', namespaces=namespaces)[0] if item.xpath('.//g:custom_label_1/text()', namespaces=namespaces) else \"\",\n",
    "                'custom_label_2': item.xpath('.//g:custom_label_2/text()', namespaces=namespaces)[0] if item.xpath('.//g:custom_label_2/text()', namespaces=namespaces) else \"\",\n",
    "                'shipping_country': item.xpath('.//g:shipping/g:country/text()', namespaces=namespaces)[0] if item.xpath('.//g:shipping/g:country/text()', namespaces=namespaces) else \"\",\n",
    "                'shipping_service': item.xpath('.//g:shipping/g:service/text()', namespaces=namespaces)[0] if item.xpath('.//g:shipping/g:service/text()', namespaces=namespaces) else \"\",\n",
    "                'shipping_price': item.xpath('.//g:shipping/g:price/text()', namespaces=namespaces)[0] if item.xpath('.//g:shipping/g:price/text()', namespaces=namespaces) else \"\"\n",
    "            }\n",
    "\n",
    "            # Write the extracted data to the TSV file\n",
    "            tsv_file.write('\\t'.join([data[field] for field in headers]) + '\\n')\n",
    "\n",
    "# The function has been updated to exclude tags that start with 'c:'\n",
    "# \"Function updated_xml_to_tsv defined successfully.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4116035a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samsung_uk_2024-11-18_PLA file has been downloaded successfully\n",
      "vodafone_2024-11-18_PLA file has been downloaded successfully\n",
      "homebase_2024-11-18_PLA file has been downloaded successfully\n"
     ]
    }
   ],
   "source": [
    "for i in adv_lst:\n",
    "    if not os.path.exists(i):\n",
    "        os.mkdir(i)\n",
    "    if not os.path.exists('log'):\n",
    "            os.mkdir('log')\n",
    "            \n",
    "    if i == 'samsung-uk':\n",
    "        lst = open(i+'-ftp.txt','r').readlines()\n",
    "        username = lst[0].split(':')[1].split('\\n')[0]\n",
    "        password = lst[1].split(':')[1]\n",
    "        path = './'+i+'/'\n",
    "        file = (path+'samsung_pla_copy-'+str(date.today()) +'.tsv')\n",
    "        #check if exists\n",
    "        if os.path.exists(file):\n",
    "            print(i+' PLA file is already downloaded')\n",
    "            continue\n",
    "            \n",
    "        ftp = ftplib.FTP('ftp2.feedonomics.com', username, password)\n",
    "        \n",
    "        with open(file, \"wb\") as f:\n",
    "            ftp.retrbinary('RETR '+ 'exports/samsung_pla_copy.txt', f.write)\n",
    "            print('samsung_uk_'+str(date.today())+'_PLA file has been downloaded successfully')\n",
    "            \n",
    "        fi = invalid_clean.MyClass()\n",
    "        lst = fi.readlogfile(file)\n",
    "        \n",
    "        with open('./log/'+i+'_'+str(date.today())+'_PLA_invalid_char_removal.log','w',encoding = 'utf_8_sig') as f:\n",
    "            for j in lst:\n",
    "                f.writelines(j+'\\n')\n",
    "        \n",
    "        out = open(file.replace('.tsv','_cleaned.tsv'),'w')\n",
    "        for line in fi._data_bytes: \n",
    "            out.write(line.replace('\\r','')) \n",
    "        out.close()\n",
    "        \n",
    "    elif i == 'vodafone':\n",
    "        path = './'+i+'/'\n",
    "        file = (path+'google_pla_vodafone_consumer_'+str(date.today()) +'.tsv')\n",
    "        #check if exists\n",
    "        if os.path.exists(file):\n",
    "            print(i+' PLA file is already downloaded')\n",
    "            continue\n",
    "        \n",
    "#         res = requests.get('http://livedata.bigupdata.co.uk/google_pla_vodafone_consumer.txt')  \n",
    "        res = requests.get('https://feed-download.bigupdata.co.uk/download/?lnk=0ef2b49a301a46c29ff0acf5a913ad75')  \n",
    "         \n",
    "        with open (file,'w',encoding='utf_8_sig') as f:\n",
    "            f.writelines(res.text)\n",
    "        fi = invalid_clean.MyClass()\n",
    "        lst = fi.readlogfile(file)\n",
    "        with open('./log/'+i+'_'+str(date.today())+'_PLA_invalid_char_removal.log','w',encoding = 'utf_8_sig') as f:\n",
    "            for j in lst:\n",
    "                f.writelines(j+'\\n')\n",
    "        print('vodafone_'+str(date.today())+'_PLA file has been downloaded successfully')\n",
    "        out = open(file.replace('.tsv','_cleaned.tsv'),'w')\n",
    "        for line in fi._data_bytes: \n",
    "            out.write(line.replace('\\r','')) \n",
    "        out.close()\n",
    "        \n",
    "    elif i == 'homebase':\n",
    "        path = './'+i+'/'\n",
    "        file = (path+'google_pla_homebase_consumer_'+str(date.today()) +'.tsv')\n",
    "        #check if exists\n",
    "        if os.path.exists(file):\n",
    "            print(i+' PLA file is already downloaded')\n",
    "            continue\n",
    "        \n",
    "#         res = requests.get('http://livedata.bigupdata.co.uk/google_pla_vodafone_consumer.txt')\n",
    "        res = requests.get('https://s2.feedhero.net/output_feeds/gb/homebase_gb/194c1b066786e88bc5f6cc2633623e5b/latest.xml')  \n",
    "         \n",
    "        tsv_filename = file\n",
    "        updated_xml_to_tsv(res.text, tsv_filename)\n",
    "        \n",
    "        fi = invalid_clean.MyClass()\n",
    "        lst = fi.readlogfile(file)\n",
    "        with open('./log/'+i+'_'+str(date.today())+'_PLA_invalid_char_removal.log','w',encoding = 'utf_8_sig') as f:\n",
    "            for j in lst:\n",
    "                f.writelines(j+'\\n')\n",
    "        print('homebase_'+str(date.today())+'_PLA file has been downloaded successfully')\n",
    "        out = open(file.replace('.tsv','_cleaned.tsv'),'w')\n",
    "        for line in fi._data_bytes: \n",
    "            out.write(line.replace('\\r','')) \n",
    "        out.close()\n",
    "        \n",
    "#     else:\n",
    "#         # Load raw PLA file from ftp\n",
    "#         lst = open(i+'-ftp.txt','r').readlines()\n",
    "#         username = lst[0].split(':')[1].split('\\n')[0]\n",
    "#         password = lst[1].split(':')[1]\n",
    "\n",
    "#         if os.path.exists(localpath):\n",
    "#             print(i+' PLA file is already downloaded')\n",
    "#             continue\n",
    "#         # Open a transport\n",
    "#         host,port = \"ftp.admarketplace.net\",8022\n",
    "#         transport = paramiko.Transport((host,port))\n",
    "#         # Auth    \n",
    "#         username,password = username,password\n",
    "#         transport.connect(None,username,password)\n",
    "#         # Go!    \n",
    "#         sftp = paramiko.SFTPClient.from_transport(transport)\n",
    "#         # Download\n",
    "#         sftp.get(filepath,localpath)\n",
    "#         print(i+'_'+str(date.today())+'_PLA file has been downloaded successfully')\n",
    "#         if sftp: sftp.close()\n",
    "#         if transport: transport.close()\n",
    "\n",
    "#         fi = invalid_clean.MyClass()\n",
    "#         lst = fi.readlogfile(localpath)\n",
    "        \n",
    "#         with open('./log/'+i+'_'+str(date.today())+'_PLA_invalid_char_removal.log','w',encoding = 'utf_8_sig') as f:\n",
    "#             for j in lst:\n",
    "#                 f.writelines(j+'\\n')\n",
    "        \n",
    "#         out = open(localpath.replace('.csv','_cleaned.csv'),'w')\n",
    "#         for line in fi._data_bytes: \n",
    "#             out.write(line.replace('\\r','')) \n",
    "#         out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2928080b",
   "metadata": {},
   "source": [
    "## Kelkoo PLA Transfer & Upload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc32fffb",
   "metadata": {},
   "source": [
    "### Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1da979a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(adv):\n",
    "    files = os.listdir(adv)\n",
    "    print(files)\n",
    "    \n",
    "    for file in files:\n",
    "        if str(date.today()) in file and 'cleaned' in file:\n",
    "            path = file\n",
    "            print(path)\n",
    "            \n",
    "    if '.tsv' in path or 'txt' in path:\n",
    "        df = pd.read_csv('./'+adv+'/'+path,sep='\\t',encoding='utf_8_sig', dtype = str)\n",
    "    else:\n",
    "        df = pd.read_csv('./'+adv+'/'+path,encoding='utf_8_sig', dtype = str)\n",
    "#         df = pd.read_csv('./'+adv+'/'+path,encoding='utf_8_sig', dtype = str).drop('Unnamed: 0',1)\n",
    "    df.columns = [x.lower() for x in list(df.columns)]\n",
    "    if 'unnamed: 0' in df.columns:\n",
    "        df = df.drop('unnamed: 0',1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "010cb188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kelkoo_samsung-uk_2024-10-15_PLA.csv', 'kelkoo_samsung-uk_2024-10-22_PLA.csv', 'kelkoo_samsung-uk_2024-10-28_PLA.csv', 'kelkoo_samsung-uk_2024-11-07_PLA.csv', 'samsung_pla_copy-2024-10-15.tsv', 'samsung_pla_copy-2024-10-15_cleaned.tsv', 'samsung_pla_copy-2024-10-22.tsv', 'samsung_pla_copy-2024-10-22_cleaned.tsv', 'samsung_pla_copy-2024-10-28.tsv', 'samsung_pla_copy-2024-10-28_cleaned.tsv', 'samsung_pla_copy-2024-11-06.tsv', 'samsung_pla_copy-2024-11-06_cleaned.tsv', 'samsung_pla_copy-2024-11-07.tsv', 'samsung_pla_copy-2024-11-07_cleaned.tsv', 'samsung_pla_copy-2024-11-18.tsv', 'samsung_pla_copy-2024-11-18_cleaned.tsv']\n",
      "samsung_pla_copy-2024-11-18_cleaned.tsv\n",
      "samsung-uk is transferred and saved!\n",
      "\n",
      "['Google _ Shopping _ Vodafone.txt', 'google_pla_vodafone_consumer_2024-10-15.tsv', 'google_pla_vodafone_consumer_2024-10-15_cleaned.tsv', 'google_pla_vodafone_consumer_2024-10-22.tsv', 'google_pla_vodafone_consumer_2024-10-22_cleaned.tsv', 'google_pla_vodafone_consumer_2024-10-28.tsv', 'google_pla_vodafone_consumer_2024-10-28_cleaned.tsv', 'google_pla_vodafone_consumer_2024-11-06.tsv', 'google_pla_vodafone_consumer_2024-11-06_cleaned.tsv', 'google_pla_vodafone_consumer_2024-11-07.tsv', 'google_pla_vodafone_consumer_2024-11-07_cleaned.tsv', 'google_pla_vodafone_consumer_2024-11-18.tsv', 'google_pla_vodafone_consumer_2024-11-18_cleaned.tsv', 'kelkoo_vodafone_2024-10-15_PLA.csv', 'kelkoo_vodafone_2024-10-22_PLA.csv', 'kelkoo_vodafone_2024-10-28_PLA.csv', 'kelkoo_vodafone_2024-11-07_PLA.csv']\n",
      "google_pla_vodafone_consumer_2024-11-18_cleaned.tsv\n",
      "This number is over 13 digits: 08806094733310\n",
      "This number is over 13 digits: 08806094733259\n",
      "This number is over 13 digits: 08806094733372\n",
      "This number is over 13 digits: 08806094733259\n",
      "This number is over 13 digits: 08806094724677\n",
      "This number is over 13 digits: 08806094724783\n",
      "This number is over 13 digits: 08806094724981\n",
      "This number is over 13 digits: 08806094724882\n",
      "This number is over 13 digits: 08806094733242\n",
      "This number is over 13 digits: 08806094733372\n",
      "This number is over 13 digits: 08806094724981\n",
      "This number is over 13 digits: 08806094733310\n",
      "This number is over 13 digits: 08806094733242\n",
      "This number is over 13 digits: 08806094724677\n",
      "This number is over 13 digits: 08806094724783\n",
      "This number is over 13 digits: 08806094733310\n",
      "This number is over 13 digits: 08806094724677\n",
      "This number is over 13 digits: 08806094733372\n",
      "This number is over 13 digits: 08806094733259\n",
      "This number is over 13 digits: 08806094733242\n",
      "This number is over 13 digits: 08806094724882\n",
      "This number is over 13 digits: 08806094724882\n",
      "This number is over 13 digits: 08806094724981\n",
      "This number is over 13 digits: 08806094724783\n",
      "vodafone is transferred and saved!\n",
      "\n",
      "['google_pla_homebase_consumer_2024-10-15.tsv', 'google_pla_homebase_consumer_2024-10-15_cleaned.tsv', 'google_pla_homebase_consumer_2024-10-22.tsv', 'google_pla_homebase_consumer_2024-10-22_cleaned.tsv', 'google_pla_homebase_consumer_2024-10-28.tsv', 'google_pla_homebase_consumer_2024-10-28_cleaned.tsv', 'google_pla_homebase_consumer_2024-11-06.tsv', 'google_pla_homebase_consumer_2024-11-06_cleaned.tsv', 'google_pla_homebase_consumer_2024-11-07.tsv', 'google_pla_homebase_consumer_2024-11-07_cleaned.tsv', 'google_pla_homebase_consumer_2024-11-18.tsv', 'google_pla_homebase_consumer_2024-11-18_cleaned.tsv', 'kelkoo_homebase_2024-10-15_PLA.csv', 'kelkoo_homebase_2024-10-22_PLA.csv', 'kelkoo_homebase_2024-10-28_PLA.csv', 'kelkoo_homebase_2024-11-07_PLA.csv']\n",
      "google_pla_homebase_consumer_2024-11-18_cleaned.tsv\n",
      "Null Value\n",
      "Null Value\n",
      "Null Value\n",
      "Null Value\n",
      "Null Value\n",
      "Null Value\n",
      "homebase has no mobile link column\n",
      "No sale price column found for homebase!\n",
      "homebase is transferred and saved!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for adv in adv_lst:\n",
    "    DL_url = []\n",
    "    DL_mobile_url = []\n",
    "    df = read_file(adv)\n",
    "    \n",
    "#     if df is None:\n",
    "#         print(f\"Error: No data returned for {adv}\")\n",
    "#     continue\n",
    "    \n",
    "    # Filter 'Out of stock' assetsFcat\n",
    "    availability_col = 'availability' if 'availability' in df.columns else 'Availability'\n",
    "    product_url_col = 'product url' if 'product url' in df.columns else 'link'\n",
    "    mobile_link_col = 'mobile link' if 'mobile link' in df.columns else 'mobile_link'\n",
    "    \n",
    "    df_in_stock = df[df[availability_col] != 'Out of Stock']\n",
    "    df_in_stock = df_in_stock[~df_in_stock[product_url_col].isnull()].reset_index(drop=True)\n",
    "    \n",
    "    # Gtin handle\n",
    "    if adv != 'samsung-uk' and 'gtin' in df_in_stock.columns:\n",
    "        df_in_stock['gtin'] = [str(gtin_processor(x)) for x in df_in_stock['gtin']]\n",
    "        \n",
    "    dic = {}  # This seems to be for mapping category with sub-values, but the logic is not clear in the provided code.\n",
    "    \n",
    "    root_url = 'https://publisher_name.ampxdirect.com/'\n",
    "    \n",
    "    if adv == 'samsung-uk':\n",
    "        sub3 = str(adv.split('-')[-1]).replace('uk','gb')\n",
    "        adv_name = str(adv.split('-')[0])\n",
    "        DL_url = [kelkoo_transfer.gen_deeplink('kelkoo',root_url,cat,cu=url,fbu=url,sub1='googleshopping',sub2=adv_name,sub3=sub3,adv=adv_name,dic=dic) for cat,url in zip(df_in_stock['category'],df_in_stock['product url'])]\n",
    "    \n",
    "    elif adv =='vodafone':\n",
    "        DL_url = [kelkoo_transfer.gen_deeplink('kelkoo',root_url,cat,cu=url,fbu=url,sub1='googleshopping',sub2=adv,sub3='gb',sub4='desktop',adv=adv,dic=dic) for cat,url in zip(df_in_stock['google_product_category'],df_in_stock['link'])]\n",
    "#         DL_url = [kelkoo_transfer.gen_deeplink('kelkoo',root_url,cu=url,fbu=url,sub1='googleshopping',sub2=adv,sub3='gb',sub4='desktop',adv=adv,dic=dic) for url in df_in_stock['link']]\n",
    "        #DL_mobile_url = [kelkoo_transfer.gen_deeplink('kelkoo',root_url,cat,cu=url,fbu=url,sub1='googleshopping',sub2=adv,sub3='gb',adv=adv,dic=dic) if url==url else '' for cat,url in zip(df_in_stock['google_product_category'],df_in_stock['mobile_link'])]\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            DL_url = [kelkoo_transfer.gen_deeplink('kelkoo',root_url,cat,cu=url,fbu=url,sub1='googleshopping',sub2=adv,sub3='gb',adv=adv,dic=dic) if url==url else '' for cat,url in zip(df_in_stock['google_product_category'],df_in_stock['link'])]\n",
    "        except:\n",
    "            DL_url = [kelkoo_transfer.gen_deeplink('kelkoo',root_url,cat,cu=url,fbu=url,sub1='googleshopping',sub2=adv,sub3='gb',adv=adv,dic=dic) if url==url else '' for cat,url in zip(df_in_stock['google product category'],df_in_stock['link'])]\n",
    "        try:\n",
    "            DL_mobile_url = [kelkoo_transfer.gen_deeplink('kelkoo',root_url,cat,cu=url,fbu=url,sub1='googleshopping',sub2=adv,sub3='us',adv=adv,dic=dic) if url==url else '' for cat,url in zip(df_in_stock['google_product_category'],df_in_stock['mobile_link'])]\n",
    "        except:\n",
    "            try:\n",
    "                DL_mobile_url = [kelkoo_transfer.gen_deeplink('kelkoo',root_url,cat,cu=url,fbu=url,sub1='googleshopping',sub2=adv,sub3='us',adv=adv,dic=dic) if url==url else '' for cat,url in zip(df_in_stock['google product category'],df_in_stock['mobile link'])]\n",
    "            except:\n",
    "                print(adv,'has no mobile link column')\n",
    "    # ... (similar logic for deeplinks as you provided, but now use the product_url_col and mobile_link_col variables)\n",
    "    \n",
    "    # Assign deeplink value to the dataframe\n",
    "    df_in_stock[product_url_col] = pd.DataFrame(DL_url)\n",
    "    if mobile_link_col in df_in_stock.columns:\n",
    "        df_in_stock[mobile_link_col] = pd.DataFrame(DL_mobile_url)\n",
    "\n",
    "    df_final = df_in_stock[df_in_stock[product_url_col] != ''].reset_index(drop=True)\n",
    "    df_final = df_final.fillna('NULL')\n",
    "    \n",
    "    # Handle sale price\n",
    "    sale_price_col = 'sale_price' if 'sale_price' in df_final.columns else 'sale price'\n",
    "    if sale_price_col in df_final.columns:\n",
    "        df_final[sale_price_col] = [i if y == 'NULL' else y for i, y in zip(df_final['price'], df_final[sale_price_col])]\n",
    "    else:\n",
    "        print(f\"No sale price column found for {adv}!\")\n",
    "    \n",
    "    # Save csv file to the local drive\n",
    "    path = f'./{adv}/'\n",
    "    df_final.to_csv(path + f'kelkoo_{adv}_{date.today()}_PLA.csv', encoding='utf_8_sig', float_format=str)\n",
    "    print(f\"{adv} is transferred and saved!\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cfa540",
   "metadata": {},
   "source": [
    "## Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf043cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the directory to where the text file which stores ftp credentials\n",
    "\n",
    "lst = open('kelkoo-ftp.txt','r').readlines()\n",
    "username = lst[0].split(':')[1].split('\\n')[0]\n",
    "password = lst[1].split(':')[1]\n",
    "ftp = ftplib.FTP('ftpkelkoo.kelkoo.net', username, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3635893e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--    1 12668    502      36831452 Nov 07 15:27 AMP_kelkoo_homebase_US.csv\n",
      "-rw-r--r--    1 12668    502        971471 Nov 07 15:26 AMP_kelkoo_vodafone_UK.csv\n",
      "-rw-r--r--    1 12668    502       3502104 Nov 07 15:26 AMP_kelkoo_samsung_UK.csv\n",
      "-rw-r--r--    1 12668    502      38485125 Aug 15  2023 AMP_kelkoo_CalvinKlein_GB.csv\n",
      "-rw-r--r--    1 12668    502       1503167 Aug 15  2023 AMP_kelkoo_TheBodyShop_GB.csv\n",
      "-rw-r--r--    1 12668    502        288053 Jul 27  2023 AMP_kelkoo_verizon_US.csv\n",
      "-rw-r--r--    1 12668    502        583638 Jul 27  2023 AMP_kelkoo_casper_US.csv\n",
      "-rw-r--r--    1 12668    502       9579740 Jul 27  2023 AMP_kelkoo_reebok_US.csv\n",
      "-rw-r--r--    1 12668    502      30843559 May 25  2023 AMP_kelkoo_homebase_UK.csv\n",
      "-rw-r--r--    1 12668    502      31413179 Feb 07  2023 AMP_kelkoo_underarmour_UK.csv\n",
      "-rw-r--r--    1 12668    502      35221872 Nov 14  2022 AMP_kelkoo_boots_UK.csv\n",
      "-rw-r--r--    1 12668    502          5171 Aug 08  2022 AMP_kelkoo_voxi_UK.csv\n",
      "-rw-r--r--    1 12668    502       1684281 May 26  2022 AMP_kelkoo_thebodyshop_FR.csv\n",
      "-rw-r--r--    1 12668    502      48467276 May 20  2022 AMP_kelkoo_underarmour_DE.csv\n",
      "-rw-r--r--    1 12668    502       1292011 May 16  2022 AMP_kelkoo_thebodyshop_DE.csv\n",
      "-rw-r--r--    1 12668    502      29287888 Mar 16  2022 AMP_kelkoo_ariat_UK.csv\n",
      "-rw-r--r--    1 12668    502       1531268 Feb 28  2022 AMP_kelkoo_thebodyshop_UK.csv\n",
      "-rw-r--r--    1 12668    502      389110791 Feb 23  2022 AMP_kelkoo_adidas_UK.csv\n",
      "-rw-r--r--    1 12668    502      371612056 Feb 23  2022 AMP_kelkoo_adidas_FR.csv\n",
      "-rw-r--r--    1 12668    502      364922767 Feb 23  2022 AMP_kelkoo_adidas_DE.csv\n",
      "-rw-r--r--    1 12668    502           944 Aug 18  2021 AMP_kelkoo_Avanquest_UK.csv\n",
      "-rw-r--r--    1 12668    502           944 Aug 18  2021 AMP_kelkoo_Avanquest_US.csv\n",
      "-rw-r--r--    1 12668    502           953 Aug 18  2021 AMP_kelkoo_Avanquest_DE.csv\n",
      "-rw-r--r--    1 12668    502        246175 Jul 20  2021 AMP_kelkoo_purple_US.csv\n",
      "-rw-r--r--    1 12668    502      16318586 Apr 21  2021 AMP_kelkoo_levis_IT.csv\n",
      "-rw-r--r--    1 12668    502      22865319 Apr 21  2021 AMP_kelkoo_levis_FR.csv\n",
      "-rw-r--r--    1 12668    502      23066404 Apr 21  2021 AMP_kelkoo_levis_DE.csv\n",
      "-rw-r--r--    1 12668    502          5880 Mar 16  2021 AMP_kelkoo_totalav_CA.csv\n",
      "-rw-r--r--    1 12668    502          5880 Mar 16  2021 AMP_kelkoo_totalav_AU.csv\n",
      "-rw-r--r--    1 12668    502          5900 Mar 16  2021 AMP_ kelkoo_totalav_FR.csv\n",
      "-rw-r--r--    1 12668    502          5880 Mar 16  2021 AMP_kelkoo_totalav_US.csv\n",
      "-rw-r--r--    1 12668    502          5900 Mar 16  2021 AMP_kelkoo_totalav_DE.csv\n",
      "-rw-r--r--    1 12668    502          5900 Mar 16  2021 AMP_kelkoo_totalav_UK.csv\n"
     ]
    }
   ],
   "source": [
    "ftp.dir('-t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65a233a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./samsung-uk/kelkoo_samsung-uk_2024-11-18_PLA.csv\n",
      "samsung-uk_2024-11-18_PLA.csv has been uploaded successfully\n",
      "./vodafone/kelkoo_vodafone_2024-11-18_PLA.csv\n",
      "vodafone_2024-11-18_PLA.csv has been uploaded successfully\n",
      "./homebase/kelkoo_homebase_2024-11-18_PLA.csv\n",
      "homebase_2024-11-18_PLA.csv has been uploaded successfully\n"
     ]
    }
   ],
   "source": [
    "for adv in adv_lst: \n",
    "#     if 'levis' in adv:\n",
    "#         path = './levis/'+adv+'/'\n",
    "#         file_name = path + 'kelkoo_'+adv + '_' + str(date.today())+'_PLA.csv'\n",
    "#     else:\n",
    "    path = './'+adv+'/'\n",
    "    file_name = path + 'kelkoo_'+adv + '_' + str(date.today())+'_PLA.csv'\n",
    "    print(file_name)\n",
    "    if adv == 'vodafone':\n",
    "        with open(file_name, \"rb\") as f: \n",
    "#             print('AMP_kelkoo_'+adv+'_UK.csv')\n",
    "            ftp.storbinary('STOR ' + 'AMP_kelkoo_'+adv+'_UK.csv', f)\n",
    "#         with open(file_name.replace('vodafone_','vodafone_payg_'), \"rb\") as f:\n",
    "#             ftp.storbinary('STOR ' + 'AMP_kelkoo_'+adv+'payg_UK.csv', f)\n",
    "    elif 'samsung-uk' in adv:\n",
    "        with open(file_name, \"rb\") as f:\n",
    "            ftp.storbinary('STOR ' + 'AMP_kelkoo_samsung_'+str(adv.split('-')[-1]).upper()+'.csv', f)\n",
    "#     elif 'levis' in adv:\n",
    "#         with open(file_name, \"rb\") as f:\n",
    "# #             print('AMP_kelkoo_levis_'+str(adv.split('-')[-1]).upper()+'.csv')\n",
    "#             ftp.storbinary('STOR ' + 'AMP_kelkoo_levis_'+str(adv.split('-')[-1]).upper()+'.csv', f)\n",
    "    else:\n",
    "        with open(file_name, \"rb\") as f:\n",
    "#             print('AMP_kelkoo_'+adv+'_US.csv')\n",
    "            ftp.storbinary('STOR ' + 'AMP_kelkoo_'+adv+'_US.csv', f)\n",
    "\n",
    "    print(adv+'_'+str(date.today())+'_PLA.csv has been uploaded successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85d2920f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--    1 12668    502      36357082 Nov 18 15:13 AMP_kelkoo_homebase_US.csv\n",
      "-rw-r--r--    1 12668    502        957376 Nov 18 15:12 AMP_kelkoo_vodafone_UK.csv\n",
      "-rw-r--r--    1 12668    502       3502104 Nov 18 15:12 AMP_kelkoo_samsung_UK.csv\n",
      "-rw-r--r--    1 12668    502      38485125 Aug 15  2023 AMP_kelkoo_CalvinKlein_GB.csv\n",
      "-rw-r--r--    1 12668    502       1503167 Aug 15  2023 AMP_kelkoo_TheBodyShop_GB.csv\n",
      "-rw-r--r--    1 12668    502        288053 Jul 27  2023 AMP_kelkoo_verizon_US.csv\n",
      "-rw-r--r--    1 12668    502        583638 Jul 27  2023 AMP_kelkoo_casper_US.csv\n",
      "-rw-r--r--    1 12668    502       9579740 Jul 27  2023 AMP_kelkoo_reebok_US.csv\n",
      "-rw-r--r--    1 12668    502      30843559 May 25  2023 AMP_kelkoo_homebase_UK.csv\n",
      "-rw-r--r--    1 12668    502      31413179 Feb 07  2023 AMP_kelkoo_underarmour_UK.csv\n",
      "-rw-r--r--    1 12668    502      35221872 Nov 14  2022 AMP_kelkoo_boots_UK.csv\n",
      "-rw-r--r--    1 12668    502          5171 Aug 08  2022 AMP_kelkoo_voxi_UK.csv\n",
      "-rw-r--r--    1 12668    502       1684281 May 26  2022 AMP_kelkoo_thebodyshop_FR.csv\n",
      "-rw-r--r--    1 12668    502      48467276 May 20  2022 AMP_kelkoo_underarmour_DE.csv\n",
      "-rw-r--r--    1 12668    502       1292011 May 16  2022 AMP_kelkoo_thebodyshop_DE.csv\n",
      "-rw-r--r--    1 12668    502      29287888 Mar 16  2022 AMP_kelkoo_ariat_UK.csv\n",
      "-rw-r--r--    1 12668    502       1531268 Feb 28  2022 AMP_kelkoo_thebodyshop_UK.csv\n",
      "-rw-r--r--    1 12668    502      389110791 Feb 23  2022 AMP_kelkoo_adidas_UK.csv\n",
      "-rw-r--r--    1 12668    502      371612056 Feb 23  2022 AMP_kelkoo_adidas_FR.csv\n",
      "-rw-r--r--    1 12668    502      364922767 Feb 23  2022 AMP_kelkoo_adidas_DE.csv\n",
      "-rw-r--r--    1 12668    502           944 Aug 18  2021 AMP_kelkoo_Avanquest_UK.csv\n",
      "-rw-r--r--    1 12668    502           944 Aug 18  2021 AMP_kelkoo_Avanquest_US.csv\n",
      "-rw-r--r--    1 12668    502           953 Aug 18  2021 AMP_kelkoo_Avanquest_DE.csv\n",
      "-rw-r--r--    1 12668    502        246175 Jul 20  2021 AMP_kelkoo_purple_US.csv\n",
      "-rw-r--r--    1 12668    502      16318586 Apr 21  2021 AMP_kelkoo_levis_IT.csv\n",
      "-rw-r--r--    1 12668    502      22865319 Apr 21  2021 AMP_kelkoo_levis_FR.csv\n",
      "-rw-r--r--    1 12668    502      23066404 Apr 21  2021 AMP_kelkoo_levis_DE.csv\n",
      "-rw-r--r--    1 12668    502          5880 Mar 16  2021 AMP_kelkoo_totalav_CA.csv\n",
      "-rw-r--r--    1 12668    502          5880 Mar 16  2021 AMP_kelkoo_totalav_AU.csv\n",
      "-rw-r--r--    1 12668    502          5900 Mar 16  2021 AMP_ kelkoo_totalav_FR.csv\n",
      "-rw-r--r--    1 12668    502          5880 Mar 16  2021 AMP_kelkoo_totalav_US.csv\n",
      "-rw-r--r--    1 12668    502          5900 Mar 16  2021 AMP_kelkoo_totalav_DE.csv\n",
      "-rw-r--r--    1 12668    502          5900 Mar 16  2021 AMP_kelkoo_totalav_UK.csv\n"
     ]
    }
   ],
   "source": [
    "ftp.dir('-t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48155ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
